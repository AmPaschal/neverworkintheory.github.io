---
layout: post
author: Your Name
title: "Paper Title"
date: YYYY-MM-DD
categories: ["First Category", "Second Category"]
---
<div class="review">
  <p>
    Review goes here.
  </p>
</div>
<p id="Hatton1994" class="bib"><cite>Hatton1994</cite>
  L. Hatton and A. Roberts:
  "<a href="https://doi.org/10.1109/32.328993">How accurate is scientific software?</a>".
  <em>IEEE Transactions on Software Engineering</em>, 20(10), 1994,
  <a class="doi" href="https://doi.org/10.1109/32.328993">10.1109/32.328993</a>.
</p>
<blockquote class="abstract">
  This paper describes some results of what, to the authors' knowledge, is the
  largest N-version programming experiment ever performed. The object of this
  ongoing four-year study is to attempt to determine just how consistent the
  results of scientific computation really are, and, from this, to estimate
  accuracy. The experiment is being carried out in a branch of the earth
  sciences known as seismic data processing, where 15 or so independently
  developed large commercial packages that implement mathematical algorithms
  from the same or similar published specifications in the same programming
  language (Fortran) have been developed over the last 20 years. The results of
  processing the same input dataset, using the same user-specified parameters,
  for nine of these packages is reported in this paper. Finally, feedback of
  obvious flaws was attempted to reduce the overall disagreement. The results
  are deeply disturbing. Whereas scientists like to think that their code is
  accurate to the precision of the arithmetic used, in this study, numerical
  disagreement grows at around the rate of 1% in average absolute difference per
  4000 fines of implemented code, and, even worse, the nature of the
  disagreement is nonrandom. Furthermore, the seismic data processing industry
  has better than average quality standards for its software development with
  both identifiable quality assurance functions and substantial test
  datasets.
</blockquote>
<p id="Hatton1997" class="bib"><cite>Hatton1997</cite>
  L. Hatton:
  "<a href="https://doi.org/10.1007/978-1-5041-2940-4_2">The T-experiments: errors in scientific software</a>".
  In Ronald F. Boisvert (ed.):
  <em><a href="https://doi.org/10.1007/978-1-5041-2940-4_2">Quality of Numerical Software</a>.</em>
  Springer US, 1997, 978-1-5041-2940-4.
</p>
<blockquote class="abstract">
  This paper covers two very large experiments carried out concurrently between
  1990 and 1994, together known as the T-experiments. Experiment T1 had the
  objective of measuring the consistency of several million lines of scientific
  software written in C and Fortran 77 by static deep-flow analysis across many
  different industries and application areas, and experiment T2 had the
  objective of measuring the level of dynamic disagreement between independent
  implementations of the same algorithms acting on the same input data with the
  same parameters in just one of these industrial application areas. Experiment
  T1 showed that C and Fortran are riddled with statically detectable
  inconsistencies independent of the application area. For example, interface
  inconsistencies occur at the rate of one in every 7 interfaces on average in
  Fortran, and one in every 37 interfaces in C. They also show that Fortran
  components are typically 2.5 times bigger than C components, and that roughly
  30% of the Fortran population and 10% of the C population would be deemed
  untestable by any standards. Experiment T2 was even more disturbing. Whereas
  scientists like to think that their results are accurate to the precision of
  the arithmetic used, in this study, the degree of agreement gradually
  degenerated from 6 significant figures to 1 significant figure during the
  computation. The reasons for this disagreement are laid squarely at the door
  of software failure, as other possible causes are considered and
  rejected. Taken with other evidence, these two experiments suggest that the
  results of scientific calculations involving significant amounts of software
  should be taken with several large pinches of salt.
</blockquote>
<p id="Malik2019" class="bib"><cite>Malik2019</cite>
  Mashkoor Malik, Alexandre C. G. Schimel, Giuseppe Masetti, Marc Roche, Julian Le Deunf, Margaret F.J. Dolan, Jonathan Beaudoin, Jean-Marie Augustin, Travis Hamilton, and Iain Parnum:
  "<a href="https://doi.org/10.3390/geosciences9120516">Results from the First Phase of the Seafloor Backscatter Processing Software Inter-Comparison Project</a>".
  <em>Geosciences</em>, 9(12), 2019,
  <a class="doi" href="https://doi.org/10.3390/geosciences9120516">10.3390/geosciences9120516</a>.
</p>
<blockquote class="abstract">
  Seafloor backscatter mosaics are now routinely produced from multibeam
  echosounder data and used in a wide range of marine applications. However,
  large differences (&gt;5 dB) can often be observed between the mosaics
  produced by different software packages processing the same dataset. Without
  transparency of the processing pipeline and the lack of consistency between
  software packages raises concerns about the validity of the final results. To
  recognize the source(s) of inconsistency between software, it is necessary to
  understand at which stage(s) of the data processing chain the differences
  become substantial. To this end, willing commercial and academic software
  developers were invited to generate intermediate processed backscatter results
  from a common dataset, for cross-comparison. The first phase of the study
  requested intermediate processed results consisting of two stages of the
  processing sequence: the one-value-per-beam level obtained after reading the
  raw data and the level obtained after radiometric corrections but before
  compensation of the angular dependence. Both of these intermediate results
  showed large differences between software solutions. This study explores the
  possible reasons for these differences and highlights the need for
  collaborative efforts between software developers and their users to improve
  the consistency and transparency of the backscatter data processing sequence.
</blockquote>
