---
layout: post
author: Greg Wilson
title: "How Reliable is Scientific Software?"
date: 2021-09-25
categories: ["Scientific Computing", "Software Reliability"]
---
<div class="review">
  <p>
    I started reading empirical software engineering research
    because I was working with scientists
    and became embarrassed by how many of the things I did and taught
    were based on personal experience and anecdotes.
    Was agile development really better than designing things up front?
    Were some programming languages actually better than others?
    And what exactly does "better" mean in sentences like that?
    My friends and colleagues in physics, ecology, and public health
    could cite evidence to back up their professional opinions;
    all I could do was change the subject.
  </p>
  <p>
    My interests have shifted over the years,
    but software engineering and scientific computing have always been near the center,
    which makes this set of papers a double pleasure to review.
    The first,
    <cite>Hatton1994</cite>,
    is now a quartery of a century old,
    but its conclusions are still fresh.
    The authors fed the same data into nine commercial geophysical software packages
    and compared the results;
    they found that,
    "numerical disagreement grows at around the rate of 1% in average absolute difference
    per 4000 fines of implemented code,
    and, even worse, the nature of the disagreement is nonrandom"
    (i.e., the authors of different packages make similar mistakes).
  </p>
  <p>
    <cite>Hatton1997</cite> revisited this result
    while also reporting on a concurrent experiment
    that analyzed large scientific applications written in C and Fortran.
    This study found that,
    "…C and Fortran are riddled with statically detectable inconsistencies
    independent of the application area.
    For example,
    interface inconsistencies occur at the rate of one in every 7 interfaces on average in Fortran,
    and one in every 37 interfaces in C.
    They also show that…roughly 30% of the Fortran population and 10% of the C…would be deemed untestable
    by any standards."
  </p>
  <p>
    Over twenty years later,
    <cite>Malik2019</cite> and <cite>Schweinsberg2021</cite>'s findings are equally sobering.
    <cite>Malik2019</cite> compared five backscatter programs
    and found that different ones would flag different inputs as invalid,
    that each application reported different mean levels,
    and so on;
    its authors trace some of these differences in results back to different assumptions about the underlying science
    and others back to different ways of doing calculations.
  </p>
  <p>
    Rather than using existing software,
    <cite>Schweinsberg2021</cite> had multiple independent teams use the same dataset
    to set two hypotheses however they wanted.
    They found that,
    "Researchers reported radically different analyses and dispersed empirical outcomes,
    in a number of cases obtaining significant effects in opposite directions for the same research question,"
    and that,
    "decisions about how to operationalize variables explain variability in outcomes above and beyond statistical choices."
    In other words,
    the differences between how researchers translated the question into specific tests
    of specific variables extracted from the data
    materially affected their conclusions.
  </p>
  <p>
    One response to studies like these is
    to say they prove that computational science doesn't meet the standards set for experimental science.
    However,
    a lot of experimental work is just as shaky when examined closely,
    and if mathematicians insisted that every theorem proof actually had to be rigorous,
    many fewer would be published.
    The truth is that we don't know why <em>any</em> of this stuff works as well as it does—if you doubt that,
    see how much of your faith survives a course on the philosophy of science.
  </p>
  <p>
    What we <em>do</em> know is that the more open researchers are,
    the more likely their results are to be correct.
    <cite>Wicherts2011</cite> found a strong correlation between
    researchers' willingness to share their data
    and the number of errors in their analyses.
    Making the software and data used to produce results freely available
    doesn't guarantee that those results are correct,
    but not doing so ensures that such checks are impossible.
  </p>
</div>
<p id="Hatton1994" class="bib"><cite>Hatton1994</cite>
  L. Hatton and A. Roberts:
  "<a href="https://doi.org/10.1109/32.328993">How accurate is scientific software?</a>".
  <em>IEEE Transactions on Software Engineering</em>, 20(10), 1994,
  <a class="doi" href="https://doi.org/10.1109/32.328993">10.1109/32.328993</a>.
</p>
<blockquote class="abstract">
  This paper describes some results of what, to the authors' knowledge, is the
  largest N-version programming experiment ever performed. The object of this
  ongoing four-year study is to attempt to determine just how consistent the
  results of scientific computation really are, and, from this, to estimate
  accuracy. The experiment is being carried out in a branch of the earth
  sciences known as seismic data processing, where 15 or so independently
  developed large commercial packages that implement mathematical algorithms
  from the same or similar published specifications in the same programming
  language (Fortran) have been developed over the last 20 years. The results of
  processing the same input dataset, using the same user-specified parameters,
  for nine of these packages is reported in this paper. Finally, feedback of
  obvious flaws was attempted to reduce the overall disagreement. The results
  are deeply disturbing. Whereas scientists like to think that their code is
  accurate to the precision of the arithmetic used, in this study, numerical
  disagreement grows at around the rate of 1% in average absolute difference per
  4000 fines of implemented code, and, even worse, the nature of the
  disagreement is nonrandom. Furthermore, the seismic data processing industry
  has better than average quality standards for its software development with
  both identifiable quality assurance functions and substantial test
  datasets.
</blockquote>
<p id="Hatton1997" class="bib"><cite>Hatton1997</cite>
  L. Hatton:
  "<a href="https://doi.org/10.1007/978-1-5041-2940-4_2">The T-experiments: errors in scientific software</a>".
  In Ronald F. Boisvert (ed.):
  <em><a href="https://doi.org/10.1007/978-1-5041-2940-4_2">Quality of Numerical Software</a>.</em>
  Springer US, 1997, 978-1-5041-2940-4.
</p>
<blockquote class="abstract">
  This paper covers two very large experiments carried out concurrently between
  1990 and 1994, together known as the T-experiments. Experiment T1 had the
  objective of measuring the consistency of several million lines of scientific
  software written in C and Fortran 77 by static deep-flow analysis across many
  different industries and application areas, and experiment T2 had the
  objective of measuring the level of dynamic disagreement between independent
  implementations of the same algorithms acting on the same input data with the
  same parameters in just one of these industrial application areas. Experiment
  T1 showed that C and Fortran are riddled with statically detectable
  inconsistencies independent of the application area. For example, interface
  inconsistencies occur at the rate of one in every 7 interfaces on average in
  Fortran, and one in every 37 interfaces in C. They also show that Fortran
  components are typically 2.5 times bigger than C components, and that roughly
  30% of the Fortran population and 10% of the C population would be deemed
  untestable by any standards. Experiment T2 was even more disturbing. Whereas
  scientists like to think that their results are accurate to the precision of
  the arithmetic used, in this study, the degree of agreement gradually
  degenerated from 6 significant figures to 1 significant figure during the
  computation. The reasons for this disagreement are laid squarely at the door
  of software failure, as other possible causes are considered and
  rejected. Taken with other evidence, these two experiments suggest that the
  results of scientific calculations involving significant amounts of software
  should be taken with several large pinches of salt.
</blockquote>
<p id="Malik2019" class="bib"><cite>Malik2019</cite>
  Mashkoor Malik, Alexandre C. G. Schimel, Giuseppe Masetti, Marc Roche, Julian Le Deunf, Margaret F.J. Dolan, Jonathan Beaudoin, Jean-Marie Augustin, Travis Hamilton, and Iain Parnum:
  "<a href="https://doi.org/10.3390/geosciences9120516">Results from the First Phase of the Seafloor Backscatter Processing Software Inter-Comparison Project</a>".
  <em>Geosciences</em>, 9(12), 2019,
  <a class="doi" href="https://doi.org/10.3390/geosciences9120516">10.3390/geosciences9120516</a>.
</p>
<blockquote class="abstract">
  Seafloor backscatter mosaics are now routinely produced from multibeam
  echosounder data and used in a wide range of marine applications. However,
  large differences (&gt;5 dB) can often be observed between the mosaics
  produced by different software packages processing the same dataset. Without
  transparency of the processing pipeline and the lack of consistency between
  software packages raises concerns about the validity of the final results. To
  recognize the source(s) of inconsistency between software, it is necessary to
  understand at which stage(s) of the data processing chain the differences
  become substantial. To this end, willing commercial and academic software
  developers were invited to generate intermediate processed backscatter results
  from a common dataset, for cross-comparison. The first phase of the study
  requested intermediate processed results consisting of two stages of the
  processing sequence: the one-value-per-beam level obtained after reading the
  raw data and the level obtained after radiometric corrections but before
  compensation of the angular dependence. Both of these intermediate results
  showed large differences between software solutions. This study explores the
  possible reasons for these differences and highlights the need for
  collaborative efforts between software developers and their users to improve
  the consistency and transparency of the backscatter data processing sequence.
</blockquote>
<p id="Schweinsberg2021" class="bib"><cite>Schweinsberg2021</cite>
  Martin Schweinsberg and 178 others:
  "<a href="https://doi.org/10.1016/j.obhdp.2021.02.003">Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis</a>".
  <em>Organizational Behavior and Human Decision Processes</em>, 165, 2021,
  <a class="doi" href="https://doi.org/10.1016/j.obhdp.2021.02.003">10.1016/j.obhdp.2021.02.003</a>.
</p>
<blockquote class="abstract">
  In this crowdsourced initiative, independent analysts used the same dataset to
  test two hypotheses regarding the effects of scientists' gender and
  professional status on verbosity during group meetings. Not only the analytic
  approach but also the operationalizations of key variables were left
  unconstrained and up to individual analysts. For instance, analysts could
  choose to operationalize status as job title, institutional ranking, citation
  counts, or some combination. To maximize transparency regarding the process by
  which analytic choices are made, the analysts used a platform we developed
  called DataExplained to justify both preferred and rejected analytic paths in
  real time. Analyses lacking sufficient detail, reproducible code, or with
  statistical errors were excluded, resulting in 29 analyses in the final
  sample. Researchers reported radically different analyses and dispersed
  empirical outcomes, in a number of cases obtaining significant effects in
  opposite directions for the same research question. A Boba multiverse analysis
  demonstrates that decisions about how to operationalize variables explain
  variability in outcomes above and beyond statistical choices (e.g.,
  covariates). Subjective researcher decisions play a critical role in driving
  the reported empirical results, underscoring the need for open data,
  systematic robustness checks, and transparency regarding both analytic paths
  taken and not taken. Implications for organizations and leaders, whose
  decision making relies in part on scientific findings, consulting reports, and
  internal analyses by data scientists, are discussed.
</blockquote>
