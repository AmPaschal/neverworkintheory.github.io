---
layout: page
title: Reviewed
---
<p><a href="../tex/reviewed.bib">BibTeX</a></p>
<p id="Akerblom2016" class="bibliography"><span class="bibliographykey">Akerblom2016</span>
Beatrice Åkerblom and Tobias Wrigstad:
"<a href="https://doi.org/10.1145/2936313.2816717">Measuring polymorphism in Python programs</a>".
<em>ACM SIGPLAN Notices</em>, 51(2), 2016,
<a class="doi" href="https://doi.org/10.1145/2936313.2816717">10.1145/2936313.2816717</a>.
<p class="abstract">Following the increased popularity of dynamic languages and their increased use in critical software, there have been many proposals to retrofit static type system to these languages to improve possibilities to catch bugs and improve performance. A key question for any type system is whether the types should be structural, for more expressiveness, or nominal, to carry more meaning for the programmer. For retrofitted type systems, it seems the current trend is using structural types. This paper attempts to answer the question to what extent this extra expressiveness is needed, and how the possible polymorphism in dynamic code is used in practise. We study polymorphism in 36 real-world open source Python programs and approximate to what extent nominal and structural types could be used to type these programs. The study is based on collecting traces from multiple runs of the programs and analysing the polymorphic degrees of targets at more than 7 million call-sites. Our results show that while polymorphism is used in all programs, the programs are to a great extent monomorphic. The polymorphism found is evenly distributed across libraries and program-specific code and occur both during program start-up and normal execution. Most programs contain a few ``megamorphic'' call-sites where receiver types vary widely. The non-monomorphic parts of the programs can to some extent be typed with nominal or structural types, but none of the approaches can type entire programs.</p>
</p>

<p id="Altadmri2015" class="bibliography"><span class="bibliographykey">Altadmri2015</span>
Amjad Altadmri and Neil C.C. Brown:
"<a href="https://doi.org/10.1145/2676723.2677258">37 Million Compilations: Investigating Novice Programming Mistakes in Large-Scale Student Data</a>".
<em>Proceedings of the 46th ACM Technical Symposium on Computer Science Education</em>, <a class="doi" href="https://doi.org/10.1145/2676723.2677258">10.1145/2676723.2677258</a>.
<p class="abstract">Previous investigations of student errors have typically focused on samples of hundreds of students at individual institutions. This work uses a year's worth of compilation events from over 250,000 students all over the world, taken from the large Blackbox data set. We analyze the frequency, time-to-fix, and spread of errors among users, showing how these factors inter-relate, in addition to their development over the course of the year. These results can inform the design of courses, textbooks and also tools to target the most frequent (or hardest to fix) errors.</p>
</p>

<p id="Ameller2012" class="bibliography"><span class="bibliographykey">Ameller2012</span>
David Ameller, Claudia Ayala, Jordi Cabot, and Xavier Franch:
"<a href="https://doi.org/10.1109/re.2012.6345838">How do software architects consider non-functional requirements: An exploratory study</a>".
<em>2012 20th IEEE International Requirements Engineering Conference (RE)</em>, <a class="doi" href="https://doi.org/10.1109/re.2012.6345838">10.1109/re.2012.6345838</a>.
<p class="abstract">Dealing with non-functional requirements (NFRs) has posed a challenge onto software engineers for many years. Over the years, many methods and techniques have been proposed to improve their elicitation, documentation, and validation. Knowing more about the state of the practice on these topics may benefit both practitioners' and researchers' daily work. A few empirical studies have been conducted in the past, but none under the perspective of software architects, in spite of the great influence that NFRs have on daily architects' practices. This paper presents some of the findings of an empirical study based on 13 interviews with software architects. It addresses questions such as: who decides the NFRs, what types of NFRs matter to architects, how are NFRs documented, and how are NFRs validated. The results are contextualized with existing previous work.</p>
</p>

<p id="Anda2009" class="bibliography"><span class="bibliographykey">Anda2009</span>
B.C.D. Anda, D.I.K. Sjøberg, and Audris Mockus:
"<a href="https://doi.org/10.1109/tse.2008.89">Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System</a>".
<em>IEEE Transactions on Software Engineering</em>, 35(3), 2009,
<a class="doi" href="https://doi.org/10.1109/tse.2008.89">10.1109/tse.2008.89</a>.
<p class="abstract">The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. Yet, reproducibility in software engineering (SE) has not been investigated thoroughly, despite the fact that lack of reproducibility has both practical and scientific consequences. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirement specification. In a call for tender to 81 companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo reproducibilities. The contractor's costs, actual lead time, and schedule overrun of the projects had, respectively, ldquomedium,rdquo ldquohigh,rdquo and ldquolowrdquo reproducibilities. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, ldquolow,rdquo "high,rdquo and ldquolowrdquo reproducibilities. Moreover, variability for predictable reasons is also included in the notion of reproducibility. We found that the observed outcome of the four development projects matched our expectations, which were formulated partially on the basis of SE folklore. Nevertheless, achieving more reproducibility in SE remains a great challenge for SE research, education, and industry.</p>
</p>

<p id="Apel2011" class="bibliography"><span class="bibliographykey">Apel2011</span>
Sven Apel, Jörg Liebig, Benjamin Brandl, Christian Lengauer, and Christian Kästner:
"<a href="https://doi.org/10.1145/2025113.2025141">Semistructured Merge: Rethinking Merge in Revision Control Systems</a>".
<em>Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering - SIGSOFT/FSE '11</em>, <a class="doi" href="https://doi.org/10.1145/2025113.2025141">10.1145/2025113.2025141</a>.
<p class="abstract">An ongoing problem in revision control systems is how to resolve conflicts in a merge of independently developed revisions. Unstructured revision control systems are purely text-based and solve conflicts based on textual similarity. Structured revision control systems are tailored to specific languages and use language-specific knowledge for conflict resolution. We propose semistructured revision control systems that inherit the strengths of both: the generality of unstructured systems and the expressiveness of structured systems. The idea is to provide structural information of the underlying software artifacts — declaratively, in the form of annotated grammars. This way, a wide variety of languages can be supported and the information provided can assist in the automatic resolution of two classes of conflicts: ordering conflicts and semantic conflicts. The former can be resolved independently of the language and the latter using specific conflict handlers. We have been developing a tool that supports semistructured merge and conducted an empirical study on 24 software projects developed in Java, C#, and Python comprising 180 merge scenarios. We found that semistructured merge reduces the number of conflicts in 60% of the sample merge scenarios by, on average, 34%, compared to unstructured merge. We found also that renaming is challenging in that it can increase the number of conflicts during semistructured merge, and that a combination of unstructured and semistructured merge is a pragmatic way to go.</p>
</p>

<p id="Balachandran2013" class="bibliography"><span class="bibliographykey">Balachandran2013</span>
Vipin Balachandran:
"<a href="https://doi.org/10.1109/icse.2013.6606642">Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606642">10.1109/icse.2013.6606642</a>.
<p class="abstract">Peer code review is a cost-effective software defect detection technique. Tool assisted code review is a form of peer code review, which can improve both quality and quantity of reviews. However, there is a significant amount of human effort involved even in tool based code reviews. Using static analysis tools, it is possible to reduce the human effort by automating the checks for coding standard violations and common defect patterns. Towards this goal, we propose a tool called Review Bot for the integration of automatic static analysis with the code review process. Review Bot uses output of multiple static analysis tools to publish reviews automatically. Through a user study, we show that integrating static analysis tools with code review process can improve the quality of code review. The developer feedback for a subset of comments from automatic reviews shows that the developers agree to fix 93% of all the automatically generated comments. There is only 14.71% of all the accepted comments which need improvements in terms of priority, comment message, etc. Another problem with tool assisted code review is the assignment of appropriate reviewers. Review Bot solves this problem by generating reviewer recommendations based on change history of source code lines. Our experimental results show that the recommendation accuracy is in the range of 60%-92%, which is significantly better than a comparable method based on file change history.</p>
</p>

<p id="Barnett2011" class="bibliography"><span class="bibliographykey">Barnett2011</span>
Mike Barnett, Manuel Fähndrich, K. Rustan M. Leino, Peter Müller, Wolfram Schulte, and Herman Venter:
"<a href="https://doi.org/10.1145/1953122.1953145">Specification and verification: the Spec# experience</a>".
<em>Communications of the ACM</em>, 54(6), 2011,
<a class="doi" href="https://doi.org/10.1145/1953122.1953145">10.1145/1953122.1953145</a>.
<p class="abstract">Can a programming language really help programmers write better programs?</p>
</p>

<p id="Barr2012" class="bibliography"><span class="bibliographykey">Barr2012</span>
Earl T. Barr, Christian Bird, Peter C. Rigby, Abram Hindle, Daniel M. German, and Premkumar Devanbu:
"<a href="https://doi.org/10.1007/978-3-642-28872-2_22">Cohesive and Isolated Development with Branches</a>".
<em>Proceedings of the 15th international conference on Fundamental Approaches to Software Engineering</em>, <a class="doi" href="https://doi.org/10.1007/978-3-642-28872-2_22">10.1007/978-3-642-28872-2_22</a>.
<p class="abstract">The adoption of distributed version control (DVC ), such as Git and Mercurial, in open-source software (OSS) projects has been explosive. Why is this and how are projects using DVC? This new generation of version control supports two important new features: distributed repositories and histories that preserve branches and merges. Through interviews with lead developers in OSS projects and a quantitative analysis of mined data from the histories of sixty project, we find that the vast majority of the projects now using DVC continue to use a centralized model of code sharing, while using branching much more extensively than before their transition to DVC. We then examine the Linux history in depth in an effort to understand and evaluate how branches are used and what benefits they provide. We find that they enable natural collaborative processes: DVC branching allows developers to collaborate on tasks in highly cohesive branches, while enjoying reduced interference from developers working on other tasks, even if those tasks are strongly coupled to theirs.</p>
</p>

<p id="Barzilay2011" class="bibliography"><span class="bibliographykey">Barzilay2011</span>
Ohad Barzilay:
"<a href="https://doi.org/10.1145/2089131.2089135">Example embedding</a>".
<em>Proceedings of the 10th SIGPLAN symposium on New ideas, new paradigms, and reflections on programming and software - ONWARD '11</em>, <a class="doi" href="https://doi.org/10.1145/2089131.2089135">10.1145/2089131.2089135</a>.
<p class="abstract">Using code examples in professional software development is like teenage sex. Those who say they do it all the time are probably lying. Although it is natural, those who do it feel guilty. Finally, once they start doing it, they are often not too concerned with safety, they discover that it is going to take a while to get really good at it, and they realize they will have to come up with a bunch of new ways of doing it before they really figure it all out.</p>
</p>

<p id="Beck2011" class="bibliography"><span class="bibliographykey">Beck2011</span>
Fabian Beck and Stephan Diehl:
"<a href="https://doi.org/10.1145/2025113.2025162">On the congruence of modularity and code coupling</a>".
<em>Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering - SIGSOFT/FSE '11</em>, <a class="doi" href="https://doi.org/10.1145/2025113.2025162">10.1145/2025113.2025162</a>.
<p class="abstract">Software systems are modularized to make their inherent complexity manageable. While there exists a set of well-known principles that may guide software engineers to design the modules of a software system, we do not know which principles are followed in practice. In a study based on 16 open source projects, we look at different kinds of coupling concepts between source code entities, including structural dependencies, fan-out similarity, evolutionary coupling, code ownership, code clones, and semantic similarity. The congruence between these coupling concepts and the modularization of the system hints at the modularity principles used in practice. Furthermore, the results provide insights on how to support developers to modularize software systems.</p>
</p>

<p id="Beller2015" class="bibliography"><span class="bibliographykey">Beller2015</span>
Moritz Beller, Georgios Gousios, Annibale Panichella, and Andy Zaidman:
"<a href="https://doi.org/10.1145/2786805.2786843">When, how, and why developers (do not) test in their IDEs</a>".
<em>Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2786805.2786843">10.1145/2786805.2786843</a>.
<p class="abstract">The research community in Software Engineering and Software Testing in particular builds many of its contributions on a set of mutually shared expectations. Despite the fact that they form the basis of many publications as well as open-source and commercial testing applications, these common expectations and beliefs are rarely ever questioned. For example, Frederic Brooks' statement that testing takes half of the development time seems to have manifested itself within the community since he first made it in the "Mythical Man Month" in 1975. With this paper, we report on the surprising results of a large-scale field study with 416 software engineers whose development activity we closely monitored over the course of five months, resulting in over 13 years of recorded work time in their integrated development environments (IDEs). Our findings question several commonly shared assumptions and beliefs about testing and might be contributing factors to the observed bug proneness of software in practice: the majority of developers in our study does not test; developers rarely run their tests in the IDE; Test-Driven Development (TDD) is not widely practiced; and, last but not least, software developers only spend a quarter of their work time engineering tests, whereas they think they test half of their time.</p>
</p>

<p id="Beller2019" class="bibliography"><span class="bibliographykey">Beller2019</span>
Moritz Beller, Georgios Gousios, Annibale Panichella, Sebastian Proksch, Sven Amann, and Andy Zaidman:
"<a href="https://doi.org/10.1109/tse.2017.2776152">Developer Testing in the IDE: Patterns, Beliefs, and Behavior</a>".
<em>IEEE Transactions on Software Engineering</em>, 45(3), 2019,
<a class="doi" href="https://doi.org/10.1109/tse.2017.2776152">10.1109/tse.2017.2776152</a>.
<p class="abstract">Software testing is one of the key activities to achieve software quality in practice. Despite its importance, however, we have a remarkable lack of knowledge on how developers test in real-world projects. In this paper, we report on a large-scale field study with 2,443 software engineers whose development activities we closely monitored over 2.5 years in four integrated development environments (IDEs). Our findings, which largely generalized across the studied IDEs and programming languages Java and C#, question several commonly shared assumptions and beliefs about developer testing: half of the developers in our study do not test; developers rarely run their tests in the IDE; most programming sessions end without any test execution; only once they start testing, do they do it extensively; a quarter of test cases is responsible for three quarters of all test failures; 12 percent of tests show flaky behavior; Test-Driven Development (TDD) is not widely practiced; and software developers only spend a quarter of their time engineering tests, whereas they think they test half of their time. We summarize these practices of loosely guiding one's development efforts with the help of testing in an initial summary on Test-Guided Development (TGD), a behavior we argue to be closer to the development reality of most developers than TDD.</p>
</p>

<p id="BenAri2011" class="bibliography"><span class="bibliographykey">BenAri2011</span>
Mordechai Ben-Ari, Roman Bednarik, Ronit Ben-Bassat Levy, Gil Ebel, Andrés Moreno, Niko Myller, and Erkki Sutinen:
"<a href="https://doi.org/10.1016/j.jvlc.2011.04.004">A decade of research and development on program animation: The Jeliot experience</a>".
<em>Journal of Visual Languages &amp; Computing</em>, 22(5), 2011,
<a class="doi" href="https://doi.org/10.1016/j.jvlc.2011.04.004">10.1016/j.jvlc.2011.04.004</a>.
<p class="abstract">Jeliot is a program animation system for teaching and learning elementary programming that has been developed over the past decade, building on the Eliot animation system developed several years before. Extensive pedagogical research has been done on various aspects of the use of Jeliot including improvements in learning, effects on attention, and acceptance by teachers. This paper surveys this research and development, and summarizes the experience and the lessons learned.</p>
</p>

<p id="Bettenburg2008" class="bibliography"><span class="bibliographykey">Bettenburg2008</span>
Nicolas Bettenburg, Sascha Just, Adrian Schröter, Cathrin Weiss, Rahul Premraj, and Thomas Zimmermann:
"<a href="https://doi.org/10.1145/1453101.1453146">What makes a good bug report?</a>".
<em>Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering - SIGSOFT '08/FSE-16</em>, <a class="doi" href="https://doi.org/10.1145/1453101.1453146">10.1145/1453101.1453146</a>.
<p class="abstract">In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.</p>
</p>

<p id="Bird2011" class="bibliography"><span class="bibliographykey">Bird2011</span>
Christian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and Premkumar Devanbu:
"<a href="https://doi.org/10.1145/2025113.2025119">Don't touch my code!: examining the effects of ownership on software quality</a>".
<em>Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering - SIGSOFT/FSE '11</em>, <a class="doi" href="https://doi.org/10.1145/2025113.2025119">10.1145/2025113.2025119</a>.
<p class="abstract">Ownership is a key aspect of large-scale software development. We examine the relationship between different ownership measures and software failures in two large software projects: Windows Vista and Windows 7. We find that in all cases, measures of ownership such as the number of low-expertise developers, and the proportion of ownership for the top owner have a relationship with both pre-release faults and post-release failures. We also empirically identify reasons that low-expertise developers make changes to components and show that the removal of low-expertise contributions dramatically decreases the performance of contribution based defect prediction. Finally we provide recommendations for source code change policies and utilization of resources such as code inspections based on our results.</p>
</p>

<p id="Bluedorn1999" class="bibliography"><span class="bibliographykey">Bluedorn1999</span>
Allen C. Bluedorn, Daniel B. Turban, and Mary Sue Love:
"<a href="https://doi.org/10.1037/0021-9010.84.2.277">The effects of stand-up and sit-down meeting formats on meeting outcomes</a>".
<em>Journal of Applied Psychology</em>, 84(2), 1999,
<a class="doi" href="https://doi.org/10.1037/0021-9010.84.2.277">10.1037/0021-9010.84.2.277</a>.
<p class="abstract">The effects of meeting format (standing or sitting) on meeting length and the quality of group decision making were investigated by comparing meeting outcomes for 56 five-member groups that conducted meetings in a standing format with 55 five-member groups that conducted meetings in a seated format. Sit-down meetings were 34% longer than stand-up meetings, but they produced no better decisions than stand-up meetings. Significant differences were also obtained for satisfaction with the meeting and task information use during the meeting but not for synergy or commitment to the group's decision. The findings were generally congruent with meeting-management recommendations in the time-management literature, although the lack of a significant difference for decision quality was contrary to theoretical expectations. This contrary finding may have been due to differences between the temporal context in which this study was conducted and those in which other time constraint research has been conducted, thereby revealing a potentially important contingency-temporal context.</p>
</p>

<p id="Brun2011" class="bibliography"><span class="bibliographykey">Brun2011</span>
Yuriy Brun, Reid Holmes, Michael D. Ernst, and David Notkin:
"<a href="https://doi.org/10.1145/2025113.2025139">Proactive detection of collaboration conflicts</a>".
<em>Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering - SIGSOFT/FSE '11</em>, <a class="doi" href="https://doi.org/10.1145/2025113.2025139">10.1145/2025113.2025139</a>.
<p class="abstract">Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results. First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems. Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations. Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts.</p>
</p>

<p id="Chen2016" class="bibliography"><span class="bibliographykey">Chen2016</span>
Tse-Hsun Chen, Weiyi Shang, Jinqiu Yang, Ahmed E. Hassan, Michael W. Godfrey, Mohamed Nasser, and Parminder Flora:
"<a href="https://doi.org/10.1145/2901739.2901758">An empirical study on the practice of maintaining object-relational mapping code in Java systems</a>".
<em>Proceedings of the 13th International Conference on Mining Software Repositories</em>, <a class="doi" href="https://doi.org/10.1145/2901739.2901758">10.1145/2901739.2901758</a>.
<p class="abstract">Databases have become one of the most important components in modern software systems. For example, web services, cloud computing systems, and online transaction processing systems all rely heavily on databases. To abstract the complexity of accessing a database, developers make use of Object-Relational Mapping (ORM) frameworks. ORM frameworks provide an abstraction layer between the application logic and the underlying database. Such abstraction layer automatically maps objects in Object-Oriented Languages to database records, which significantly reduces the amount of boilerplate code that needs to be written. Despite the advantages of using ORM frameworks, we observe several difficulties in maintaining ORM code (i.e., code that makes use of ORM frameworks) when cooperating with our industrial partner. After conducting studies on other open source systems, we find that such difficulties are common in other Java systems. Our study finds that i) ORM cannot completely encapsulate database accesses in objects or abstract the underlying database technology, thus may cause ORM code changes more scattered; ii) ORM code changes are more frequent than regular code, but there is a lack of tools that help developers verify ORM code at compilation time; iii) we find that changes to ORM code are more commonly due to performance or security reasons; however, traditional static code analyzers need to be extended to capture the peculiarities of ORM code in order to detect such problems. Our study highlights the hidden maintenance costs of using ORM frameworks, and provides some initial insights about potential approaches to help maintain ORM code. Future studies should carefully examine ORM code, especially given the rising use of ORM in modern software systems.</p>
</p>

<p id="Cherubini2007" class="bibliography"><span class="bibliographykey">Cherubini2007</span>
Mauro Cherubini, Gina Venolia, Rob DeLine, and Amy J. Ko:
"<a href="https://doi.org/10.1145/1240624.1240714">Let's go to the whiteboard: how and why software developers use drawings</a>".
<em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>, <a class="doi" href="https://doi.org/10.1145/1240624.1240714">10.1145/1240624.1240714</a>.
<p class="abstract">Software developers are rooted in the written form of their code, yet they often draw diagrams representing their code. Unfortunately, we still know little about how and why they create these diagrams, and so there is little research to inform the design of visual tools to support developers' work. This paper presents findings from semi-structured interviews that have been validated with a structured survey. Results show that most of the diagrams had a transient nature because of the high cost of changing whiteboard sketches to electronic renderings. Diagrams that documented design decisions were often externalized in these temporary drawings and then subsequently lost. Current visualization tools and the software development practices that we observed do not solve these issues, but these results suggest several directions for future research.</p>
</p>

<p id="Chong2007" class="bibliography"><span class="bibliographykey">Chong2007</span>
Jan Chong and Tom Hurlbutt:
"<a href="https://doi.org/10.1109/icse.2007.87">The Social Dynamics of Pair Programming</a>".
<em>29th International Conference on Software Engineering (ICSE'07)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2007.87">10.1109/icse.2007.87</a>.
<p class="abstract">This paper presents data from a four month ethnographic study of professional pair programmers from two software development teams. Contrary to the current conception of pair programmers, the pairs in this study did not hew to the separate roles of "driver" and "navigator". Instead, the observed programmers moved together through different phases of the task, considering and discussing issues at the same strategic "range " or level of abstraction and in largely the same role. This form of interaction was reinforced by frequent switches in keyboard control during pairing and the use of dual keyboards. The distribution of expertise among the members of a pair had a strong influence on the tenor of pair programming interaction. Keyboard control had a consistent secondary effect on decisionmaking within the pair. These findings have implications for software development managers and practitioners as well as for the design of software development tools.</p>
</p>

<p id="Cinneide2012" class="bibliography"><span class="bibliographykey">Cinneide2012</span>
Mel Ó Cinnéide, Laurence Tratt, Mark Harman, Steve Counsell, and Iman Hemati Moghadam:
"<a href="https://doi.org/10.1145/2372251.2372260">Experimental assessment of software metrics using automated refactoring</a>".
<em>Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '12</em>, <a class="doi" href="https://doi.org/10.1145/2372251.2372260">10.1145/2372251.2372260</a>.
<p class="abstract">A large number of software metrics have been proposed in the literature, but there is little understanding of how these metrics relate to one another. We propose a novel experimental technique, based on search-based refactoring, to assess software metrics and to explore relationships between them. Our goal is not to improve the program being refactored, but to assess the software metrics that guide the automated refactoring through repeated refactoring experiments. We apply our approach to five popular cohesion metrics using eight real-world Java systems, involving 300,000 lines of code and over 3,000 refactorings. Our results demonstrate that cohesion metrics disagree with each other in 55% of cases, and show how our approach can be used to reveal novel and surprising insights into the software metrics under investigation.</p>
</p>

<p id="CruzLemus2009" class="bibliography"><span class="bibliographykey">CruzLemus2009</span>
José A. Cruz-Lemus, Marcela Genero, M. Esperanza Manso, Sandro Morasca, and Mario Piattini:
"<a href="https://doi.org/10.1007/s10664-009-9106-z">Assessing the understandability of UML statechart diagrams with composite states—A family of empirical studies</a>".
<em>Empirical Software Engineering</em>, 14(6), 2009,
<a class="doi" href="https://doi.org/10.1007/s10664-009-9106-z">10.1007/s10664-009-9106-z</a>.
<p class="abstract">The main goal of this work is to present a family of empirical studies that we have carried out to investigate whether the use of composite states may improve the understandability of UML statechart diagrams derived from class diagrams. Our hypotheses derive from conventional wisdom, which says that hierarchical modeling mechanisms are helpful in mastering the complexity of a software system. In our research, we have carried out three empirical studies, consisting of five experiments in total. The studies differed somewhat as regards the size of the UML statechart models, though their size and the complexity of the models were chosen so that they could be analyzed by the subjects within a limited time period. The studies also differed with respect to the type of subjects (students vs. professionals), the familiarity of the subjects with the domains of the diagrams, and other factors. To integrate the results obtained from each of the five experiments, we performed a meta-analysis study which allowed us to take into account the differences between studies and to obtain the overall effect that the use of composite states has on the understandability of UML statechart diagrams throughout all the experiments. The results obtained are not completely conclusive. They cast doubts on the usefulness of composite states for a better understanding and memorizing of UML statechart diagrams. Composite states seem only to be helpful for acquiring knowledge from the diagrams. At any rate, it should be noted that these results are affected by the previous experience of the subjects on modeling, as well as by the size and complexity of the UML statechart diagrams we used, so care should be taken when generalizing our results.</p>
</p>

<p id="Dabbish2012" class="bibliography"><span class="bibliographykey">Dabbish2012</span>
Laura Dabbish, Colleen Stuart, Jason Tsay, and Jim Herbsleb:
"<a href="https://doi.org/10.1145/2145204.2145396">Social coding in GitHub: transparency and collaboration in an open software repository</a>".
<em>Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work - CSCW '12</em>, <a class="doi" href="https://doi.org/10.1145/2145204.2145396">10.1145/2145204.2145396</a>.
<p class="abstract">Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation.</p>
</p>

<p id="Dagenais2010" class="bibliography"><span class="bibliographykey">Dagenais2010</span>
Barthélémy Dagenais and Martin P. Robillard:
"<a href="https://doi.org/10.1145/1882291.1882312">Creating and evolving developer documentation</a>".
<em>Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering - FSE '10</em>, <a class="doi" href="https://doi.org/10.1145/1882291.1882312">10.1145/1882291.1882312</a>.
<p class="abstract">Developer documentation helps developers learn frameworks and libraries. To better understand how documentation in open source projects is created and maintained, we performed a qualitative study in which we interviewed core contributors who wrote developer documentation and developers who read documentation. In addition, we studied the evolution of 19 documents by analyzing more than 1500 document revisions. We identified the decisions that contributors make, the factors influencing these decisions and the consequences for the project. Among many findings, we observed how working on the documentation could improve the code quality and how constant interaction with the projects' community positively impacted the documentation.</p>
</p>

<p id="Dang2012" class="bibliography"><span class="bibliographykey">Dang2012</span>
Yingnong Dang, Rongxin Wu, Hongyu Zhang, Dongmei Zhang, and Peter Nobel:
"<a href="https://doi.org/10.1109/icse.2012.6227111">ReBucket: A method for clustering duplicate crash reports based on call stack similarity</a>".
<em>2012 34th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2012.6227111">10.1109/icse.2012.6227111</a>.
<p class="abstract">Software often crashes. Once a crash happens, a crash report could be sent to software developers for investigation upon user permission. To facilitate efficient handling of crashes, crash reports received by Microsoft's Windows Error Reporting (WER) system are organized into a set of "buckets". Each bucket contains duplicate crash reports that are deemed as manifestations of the same bug. The bucket information is important for prioritizing efforts to resolve crashing bugs. To improve the accuracy of bucketing, we propose ReBucket, a method for clustering crash reports based on call stack matching. ReBucket measures the similarities of call stacks in crash reports and then assigns the reports to appropriate buckets based on the similarity values. We evaluate ReBucket using crash data collected from five widely-used Microsoft products. The results show that ReBucket achieves better overall performance than the existing methods. On average, the F-measure obtained by ReBucket is about 0.88.</p>
</p>

<p id="DeLucia2009" class="bibliography"><span class="bibliographykey">DeLucia2009</span>
Andrea De Lucia, Carmine Gravino, Rocco Oliveto, and Genoveffa Tortora:
"<a href="https://doi.org/10.1007/s10664-009-9127-7">An experimental comparison of ER and UML class diagrams for data modelling</a>".
<em>Empirical Software Engineering</em>, 15(5), 2009,
<a class="doi" href="https://doi.org/10.1007/s10664-009-9127-7">10.1007/s10664-009-9127-7</a>.
<p class="abstract">We present the results of three sets of controlled experiments aimed at analysing whether UML class diagrams are more comprehensible than ER diagrams during data models maintenance. In particular, we considered the support given by the two notations in the comprehension and interpretation of data models, comprehension of the change to perform to meet a change request, and detection of defects contained in a data model. The experiments involved university students with different levels of ability and experience. The results demonstrate that using UML class diagrams subjects achieved better comprehension levels. With regard to the support given by the two notations during maintenance activities the results demonstrate that the two notations give the same support, while in general UML class diagrams provide a better support with respect to ER diagrams during verification activities.</p>
</p>

<p id="Dzidek2008" class="bibliography"><span class="bibliographykey">Dzidek2008</span>
W.J. Dzidek, E. Arisholm, and L.C. Briand:
"<a href="https://doi.org/10.1109/tse.2008.15">A Realistic Empirical Evaluation of the Costs and Benefits of UML in Software Maintenance</a>".
<em>IEEE Transactions on Software Engineering</em>, 34(3), 2008,
<a class="doi" href="https://doi.org/10.1109/tse.2008.15">10.1109/tse.2008.15</a>.
<p class="abstract">The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54% increase in the functional correctness of changes (p=0.03), and an insignificant 7% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56%) - at the expense of an insignificant 14% increase in development time caused by the overhead of updating the UML documentation (p=0.35).</p>
</p>

<p id="Eichberg2015" class="bibliography"><span class="bibliographykey">Eichberg2015</span>
Michael Eichberg, Ben Hermann, Mira Mezini, and Leonid Glanz:
"<a href="https://doi.org/10.1145/2786805.2786865">Hidden truths in dead software paths</a>".
<em>Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2786805.2786865">10.1145/2786805.2786865</a>.
<p class="abstract">Approaches and techniques for statically finding a multitude of issues in source code have been developed in the past. A core property of these approaches is that they are usually targeted towards finding only a very specific kind of issue and that the effort to develop such an analysis is significant. This strictly limits the number of kinds of issues that can be detected. In this paper, we discuss a generic approach based on the detection of infeasible paths in code that can discover a wide range of code smells ranging from useless code that hinders comprehension to real bugs. Code issues are identified by calculating the difference between the control-flow graph that contains all technically possible edges and the corresponding graph recorded while performing a more precise analysis using abstract interpretation. We have evaluated the approach using the Java Development Kit as well as the Qualitas Corpus (a curated collection of over 100 Java Applications) and were able to find thousands of issues across a wide range of categories.</p>
</p>

<p id="ElEmam2001" class="bibliography"><span class="bibliographykey">ElEmam2001</span>
K. El Emam, S. Benlarbi, N. Goel, and S.N. Rai:
"<a href="https://doi.org/10.1109/32.935855">The confounding effect of class size on the validity of object-oriented metrics</a>".
<em>IEEE Transactions on Software Engineering</em>, 27(7), 2001,
<a class="doi" href="https://doi.org/10.1109/32.935855">10.1109/32.935855</a>.
<p class="abstract">Much effort has been devoted to the development and empirical validation of object-oriented metrics. The empirical validations performed thus far would suggest that a core set of validated metrics is close to being identified. However, none of these studies allow for the potentially confounding effect of class size. We demonstrate a strong size confounding effect and question the results of previous object-oriented metrics validation studies. We first investigated whether there is a confounding effect of class size in validation studies of object-oriented metrics and show that, based on previous work, there is reason to believe that such an effect exists. We then describe a detailed empirical methodology for identifying those effects. Finally, we perform a study on a large C++ telecommunications framework to examine if size is really a confounder. This study considered the Chidamber and Kemerer metrics and a subset of the Lorenz and Kidd metrics. The dependent variable was the incidence of a fault attributable to a field failure (fault-proneness of a class). Our findings indicate that, before controlling for size, the results are very similar to previous studies. The metrics that are expected to be validated are indeed associated with fault-proneness.</p>
</p>

<p id="Ford2016" class="bibliography"><span class="bibliographykey">Ford2016</span>
Denae Ford, Justin Smith, Philip J. Guo, and Chris Parnin:
"<a href="https://doi.org/10.1145/2950290.2950331">Paradise unplugged: identifying barriers for female participation on stack overflow</a>".
<em>Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2950290.2950331">10.1145/2950290.2950331</a>.
<p class="abstract">It is no secret that females engage less in programming fields than males. However, in online communities, such as Stack Overflow, this gender gap is even more extreme: only 5.8% of contributors are female. In this paper, we use a mixed-methods approach to identify contribution barriers females face in online communities. Through 22 semi-structured interviews with a spectrum of female users ranging from non-contributors to a top 100 ranked user of all time, we identified 14 barriers preventing them from contributing to Stack Overflow. We then conducted a survey with 1470 female and male developers to confirm which barriers are gender related or general problems for everyone. Females ranked five barriers significantly higher than males. A few of these include doubts in the level of expertise needed to contribute, feeling overwhelmed when competing with a large number of users, and limited awareness of site features. Still, there were other barriers that equally impacted all Stack Overflow users or affected particular groups, such as industry programmers. Finally, we describe several implications that may encourage increased participation in the Stack Overflow community across genders and other demographics.</p>
</p>

<p id="Fucci2016" class="bibliography"><span class="bibliographykey">Fucci2016</span>
Davide Fucci, Giuseppe Scanniello, Simone Romano, Martin Shepperd, Boyce Sigweni, Fernando Uyaguari, Burak Turhan, Natalia Juristo, and Markku Oivo:
"<a href="https://doi.org/10.1145/2961111.2962592">An External Replication on the Effects of Test-driven Development Using a Multi-site Blind Analysis Approach</a>".
<em>Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement</em>, <a class="doi" href="https://doi.org/10.1145/2961111.2962592">10.1145/2961111.2962592</a>.
<p class="abstract">Context: Test-driven development (TDD) is an agile practice claimed to improve the quality of a software product, as well as the productivity of its developers. A previous study (i.e., baseline experiment) at the University of Oulu (Finland) compared TDD to a test-last development (TLD) approach through a randomized controlled trial. The results failed to support the claims. Goal: We want to validate the original study results by replicating it at the University of Basilicata (Italy), using a different design. Method: We replicated the baseline experiment, using a crossover design, with 21 graduate students. We kept the settings and context as close as possible to the baseline experiment. In order to limit researchers bias, we involved two other sites (UPM, Spain, and Brunel, UK) to conduct blind analysis of the data. Results: The Kruskal-Wallis tests did not show any significant difference between TDD and TLD in terms of testing effort (p-value = .27), external code quality (p-value = .82), and developers' productivity (p-value = .83). Nevertheless, our data revealed a difference based on the order in which TDD and TLD were applied, though no carry over effect. Conclusions: We verify the baseline study results, yet our results raises concerns regarding the selection of experimental objects, particularly with respect to their interaction with the order in which of treatments are applied. We recommend future studies to survey the tasks used in experiments evaluating TDD. Finally, to lower the cost of replication studies and reduce researchers' bias, we encourage other research groups to adopt similar multi-site blind analysis approach described in this paper.</p>
</p>

<p id="Gauthier2013" class="bibliography"><span class="bibliographykey">Gauthier2013</span>
Francois Gauthier and Ettore Merlo:
"<a href="https://doi.org/10.1109/icse.2013.6606670">Semantic smells and errors in access control models: A case study in PHP</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606670">10.1109/icse.2013.6606670</a>.
<p class="abstract">Access control models implement mechanisms to restrict access to sensitive data from unprivileged users. Access controls typically check privileges that capture the semantics of the operations they protect. Semantic smells and errors in access control models stem from privileges that are partially or totally unrelated to the action they protect. This paper presents a novel approach, partly based on static analysis and information retrieval techniques, for the automatic detection of semantic smells and errors in access control models. Investigation of the case study application revealed 31 smells and 2 errors. Errors were reported to developers who quickly confirmed their relevance and took actions to correct them. Based on the obtained results, we also propose three categories of semantic smells and errors to lay the foundations for further research on access control smells in other systems and domains.</p>
</p>

<p id="Giger2011" class="bibliography"><span class="bibliographykey">Giger2011</span>
Emanuel Giger, Martin Pinzger, and Harald Gall:
"<a href="https://doi.org/10.1145/2024445.2024455">Using the Gini Coefficient for bug prediction in Eclipse</a>".
<em>Proceedings of the 12th international workshop and the 7th annual ERCIM workshop on Principles on software evolution and software evolution - IWPSE-EVOL '11</em>, <a class="doi" href="https://doi.org/10.1145/2024445.2024455">10.1145/2024445.2024455</a>.
<p class="abstract">The Gini coefficient is a prominent measure to quantify the inequality of a distribution. It is often used in the field of economy to describe how goods, e.g., wealth or farmland, are distributed among people. We use the Gini coefficient to measure code ownership by investigating how changes made to source code are distributed among the developer population. The results of our study with data from the Eclipse platform show that less bugs can be expected if a large share of all changes are accumulated, i.e., carried out, by relatively few developers.</p>
</p>

<p id="Gousios2016" class="bibliography"><span class="bibliographykey">Gousios2016</span>
Georgios Gousios, Margaret-Anne Storey, and Alberto Bacchelli:
"<a href="https://doi.org/10.1145/2884781.2884826">Work practices and challenges in pull-based development</a>".
<em>Proceedings of the 38th International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2884781.2884826">10.1145/2884781.2884826</a>.
<p class="abstract">The pull-based development model is an emerging way of contributing to distributed software projects that is gaining enormous popularity within the open source software (OSS) world. Previous work has examined this model by focusing on projects and their owners—we complement it by examining the work practices of project contributors and the challenges they face. We conducted a survey with 645 top contributors to active OSS projects using the pull-based model on GitHub, the prevalent social coding site. We also analyzed traces extracted from corresponding GitHub repositories. Our research shows that: contributors have a strong interest in maintaining awareness of project status to get inspiration and avoid duplicating work, but they do not actively propagate information; communication within pull requests is reportedly limited to low-level concerns and contributors often use communication channels external to pull requests; challenges are mostly social in nature, with most reporting poor responsiveness from integrators; and the increased transparency of this setting is a confirmed motivation to contribute. Based on these findings, we present recommendations for practitioners to streamline the contribution process and discuss potential future research directions.</p>
</p>

<p id="Graziotin2014" class="bibliography"><span class="bibliographykey">Graziotin2014</span>
Daniel Graziotin, Xiaofeng Wang, and Pekka Abrahamsson:
"<a href="https://doi.org/10.7717/peerj.289">Happy software developers solve problems better: psychological measurements in empirical software engineering</a>".
<em>PeerJ</em>, 2, 2014,
<a class="doi" href="https://doi.org/10.7717/peerj.289">10.7717/peerj.289</a>.
<p class="abstract">For more than thirty years, it has been claimed that a way to improve software developers' productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states—emotions and moods—deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint.</p>
</p>

<p id="Green1996" class="bibliography"><span class="bibliographykey">Green1996</span>
Thomas R. G. Green and Marian Petre:
"<a href="https://doi.org/10.1006/jvlc.1996.0009">Usability Analysis of Visual Programming Environments: A 'Cognitive Dimensions' Framework</a>".
<em>Journal of Visual Languages &amp; Computing</em>, 7(2), 1996,
<a class="doi" href="https://doi.org/10.1006/jvlc.1996.0009">10.1006/jvlc.1996.0009</a>.
<p class="abstract">Abstract The cognitive dimensions framework is a broad-brush evaluation technique for interactive devices and for non-interactive notations. It sets out a small vocabulary of terms designed to capture the cognitively-relevant aspects of structure, and shows how they can be traded off against each other. The purpose of this paper is to propose the framework as an evaluation technique for visual programming environments. We apply it to two commercially-available dataflow languages (with further examples from other systems) and conclude that it is effective and insightful; other HCI-based evaluation techniques focus on different aspects and would make good complements. Insofar as the examples we used are representative, current VPLs are successful in achieving a good 'closeness of match', but designers need to consider the 'viscosity ' (resistance to local change) and the 'secondary notation' (possibility of conveying extra meaning by choice of layout, colour, etc.).</p>
</p>

<p id="Gulzar2016" class="bibliography"><span class="bibliographykey">Gulzar2016</span>
Muhammad Ali Gulzar, Matteo Interlandi, Seunghyun Yoo, Sai Deep Tetali, Tyson Condie, Todd Millstein, and Miryung Kim:
"<a href="https://doi.org/10.1145/2884781.2884813">BigDebug: debugging primitives for interactive big data processing in Spark</a>".
<em>Proceedings of the 38th International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2884781.2884813">10.1145/2884781.2884813</a>.
<p class="abstract">Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's data-centers is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires re-thinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user.First, BigDebug's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BigDebug scales to terabytes and its record-level tracing incurs less than 25% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100% time saving compared to the baseline replay debugger. The results show that BigDebug supports debugging at interactive speeds with minimal performance impact.</p>
</p>

<p id="Hanenberg2010" class="bibliography"><span class="bibliographykey">Hanenberg2010</span>
Stefan Hanenberg:
"<a href="https://doi.org/10.1145/1869459.1869462">An experiment about static and dynamic type systems</a>".
<em>Proceedings of the ACM international conference on Object oriented programming systems languages and applications - OOPSLA '10</em>, <a class="doi" href="https://doi.org/10.1145/1869459.1869462">10.1145/1869459.1869462</a>.
<p class="abstract">Although static type systems are an essential part in teach-ing and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application's complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application's development time (under the conditions of the experiment).</p>
</p>

<p id="Hannay2010" class="bibliography"><span class="bibliographykey">Hannay2010</span>
J.E. Hannay, E. Arisholm, H. Engvik, and D.I.K. Sjøberg:
"<a href="https://doi.org/10.1109/tse.2009.41">Effects of Personality on Pair Programming</a>".
<em>IEEE Transactions on Software Engineering</em>, 36(1), 2010,
<a class="doi" href="https://doi.org/10.1109/tse.2009.41">10.1109/tse.2009.41</a>.
<p class="abstract">Personality tests in various guises are commonly used in recruitment and career counseling industries. Such tests have also been considered as instruments for predicting the job performance of software professionals both individually and in teams. However, research suggests that other human-related factors such as motivation, general mental ability, expertise, and task complexity also affect the performance in general. This paper reports on a study of the impact of the Big Five personality traits on the performance of pair programmers together with the impact of expertise and task complexity. The study involved 196 software professionals in three countries forming 98 pairs. The analysis consisted of a confirmatory part and an exploratory part. The results show that: (1) Our data do not confirm a meta-analysis-based model of the impact of certain personality traits on performance and (2) personality traits, in general, have modest predictive value on pair programming performance compared with expertise, task complexity, and country. We conclude that more effort should be spent on investigating other performance-related predictors such as expertise, and task complexity, as well as other promising predictors, such as programming skill and learning. We also conclude that effort should be spent on elaborating on the effects of personality on various measures of collaboration, which, in turn, may be used to predict and influence performance. Insights into such malleable, rather than static, factors may then be used to improve pair programming performance.</p>
</p>

<p id="Harms2016" class="bibliography"><span class="bibliographykey">Harms2016</span>
Kyle James Harms, Jason Chen, and Caitlin L. Kelleher:
"<a href="https://doi.org/10.1145/2960310.2960314">Distractors in Parsons Problems Decrease Learning Efficiency for Young Novice Programmers</a>".
<em>Proceedings of the 2016 ACM Conference on International Computing Education Research</em>, <a class="doi" href="https://doi.org/10.1145/2960310.2960314">10.1145/2960310.2960314</a>.
<p class="abstract">Parsons problems are an increasingly popular method for helping inexperienced programmers improve their programming skills. In Parsons problems, learners are given a set of programming statements that they must assemble into the correct order. Parsons problems commonly use distractors, extra statements that are not part of the solution. Yet, little is known about the effect distractors have on a learner's ability to acquire new programming skills. We present a study comparing the effectiveness of learning programming from Parsons problems with and without distractors. The results suggest that distractors decrease learning efficiency. We found that distractor participants showed no difference in transfer task performance compared to those without distractors. However, the distractors increased learners cognitive load, decreased their success at completing Parsons problems by 26%, and increased learners' time on task by 14%.</p>
</p>

<p id="Hemmati2013" class="bibliography"><span class="bibliographykey">Hemmati2013</span>
Hadi Hemmati, Sarah Nadi, Olga Baysal, Oleksii Kononenko, Wei Wang, Reid Holmes, and Michael W. Godfrey:
"<a href="https://doi.org/10.1109/msr.2013.6624048">The MSR Cookbook: Mining a decade of research</a>".
<em>2013 10th Working Conference on Mining Software Repositories (MSR)</em>, <a class="doi" href="https://doi.org/10.1109/msr.2013.6624048">10.1109/msr.2013.6624048</a>.
<p class="abstract">The Mining Software Repositories (MSR) research community has grown significantly since the first MSR workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past MSR conferences and workshops. To that end, we review all 117 full papers published in the MSR proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the MSR community can engage in a continuing discussion on our evolving best practices.</p>
</p>

<p id="Hermans2011" class="bibliography"><span class="bibliographykey">Hermans2011</span>
Felienne Hermans, Martin Pinzger, and Arie van Deursen:
"<a href="https://doi.org/10.1145/1985793.1985855">Supporting professional spreadsheet users by generating leveled dataflow diagrams</a>".
<em>Proceedings of the 33rd International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/1985793.1985855">10.1145/1985793.1985855</a>.
<p class="abstract">Thanks to their flexibility and intuitive programming model, spreadsheets are widely used in industry, often for businesscritical applications. Similar to software developers, professional spreadsheet users demand support for maintaining and transferring their spreadsheets. In this paper, we first study the problems and information needs of professional spreadsheet users by means of a survey conducted at a large financial company. Based on these needs, we then present an approach that extracts this information from spreadsheets and presents it in a compact and easy to understand way, with leveled dataflow diagrams. Our approach comes with three different views on the dataflow that allow the user to analyze the dataflow diagrams in a top-down fashion. To evaluate the usefulness of the proposed approach, we conducted a series of interviews as well as nine case studies in an industrial setting. The results of the evaluation clearly indicate the demand for and usefulness of our approach in ease the understanding of spreadsheets.</p>
</p>

<p id="Hermans2016" class="bibliography"><span class="bibliographykey">Hermans2016</span>
Felienne Hermans and Efthimia Aivaloglou:
"<a href="https://doi.org/10.1109/icpc.2016.7503706">Do code smells hamper novice programming? A controlled experiment on Scratch programs</a>".
<em>2016 IEEE 24th International Conference on Program Comprehension (ICPC)</em>, <a class="doi" href="https://doi.org/10.1109/icpc.2016.7503706">10.1109/icpc.2016.7503706</a>.
<p class="abstract">Recently, block-based programming languages like Alice, Scratch and Blockly have become popular tools for programming education. There is substantial research showing that block-based languages are suitable for early programming education. But can block-based programs be smelly too? And does that matter to learners? In this paper we explore the code smells metaphor in the context of block-based programming language Scratch. We conduct a controlled experiment with 61 novice Scratch programmers, in which we divided the novices into three groups. One third receive a non-smelly program, while the other groups receive a program suffering from the Duplication or the Long Method smell respectively. All subjects then perform the same comprehension tasks on their program, after which we measure their time and correctness. The results of the experiment show that code smell indeed influence performance: subjects working on the program exhibiting code smells perform significantly worse, but the smells did not affect the time subjects needed. Investigating different types of tasks in more detail, we find that Long Method mainly decreases system understanding, while Duplication decreases the ease with which subjects modify Scratch programs.</p>
</p>

<p id="Herzig2013" class="bibliography"><span class="bibliographykey">Herzig2013</span>
Kim Herzig, Sascha Just, and Andreas Zeller:
"<a href="https://doi.org/10.1109/icse.2013.6606585">It's not a bug, it's a feature: How misclassification impacts bug prediction</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606585">10.1109/icse.2013.6606585</a>.
<p class="abstract">In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8% of all bug reports to be misclassified - that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.</p>
</p>

<p id="Hindle2012" class="bibliography"><span class="bibliographykey">Hindle2012</span>
Abram Hindle, Christian Bird, Thomas Zimmermann, and Nachiappan Nagappan:
"<a href="https://doi.org/10.1109/icsm.2012.6405278">Relating requirements to implementation via topic analysis: Do topics extracted from requirements make sense to managers and developers?</a>".
<em>2012 28th IEEE International Conference on Software Maintenance (ICSM)</em>, <a class="doi" href="https://doi.org/10.1109/icsm.2012.6405278">10.1109/icsm.2012.6405278</a>.
<p class="abstract">Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted matches the perception that Program Managers and Developers have about the effort put into addressing certain topics. We found that effort extracted from version control that was relevant to a topic often matched the perception of the managers and developers of what occurred at the time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements.</p>
</p>

<p id="Hindle2016" class="bibliography"><span class="bibliographykey">Hindle2016</span>
Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu:
"<a href="https://doi.org/10.1145/2902362">On the naturalness of software</a>".
<em>Communications of the ACM</em>, 59(5), 2016,
<a class="doi" href="https://doi.org/10.1145/2902362">10.1145/2902362</a>.
<p class="abstract">Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.</p>
</p>

<p id="Hundhausen2011" class="bibliography"><span class="bibliographykey">Hundhausen2011</span>
Christopher D. Hundhausen, Pawan Agarwal, and Michael Trevisan:
"<a href="https://doi.org/10.1145/1953163.1953201">Online vs. face-to-face pedagogical code reviews</a>".
<em>Proceedings of the 42nd ACM technical symposium on Computer science education - SIGCSE '11</em>, <a class="doi" href="https://doi.org/10.1145/1953163.1953201">10.1145/1953163.1953201</a>.
<p class="abstract">Given the increased importance of communication, teamwork, and critical thinking skills in the computing profession, we have been exploring studio-based instructional methods, in which students develop solutions and iteratively refine them through critical review by their peers and instructor. We have developed an adaptation of studio-based instruction for computing education called the pedagogical code review (PCR), which is modeled after the code inspection process used in the software industry. Unfortunately, PCRs are time-intensive, making them difficult to implement within a typical computing course. To address this issue, we have developed an online environment that allows PCRs to take place asynchronously outside of class. We conducted an empirical study that compared a CS 1 course with online PCRs against a CS 1 course with face-to-face PCRs. Our study had three key results: (a) in the course with face-to-face PCRs, student attitudes with respect to self-efficacy and peer learning were significantly higher; (b) in the course with face-to-face PCRs, students identified more substantive issues in their reviews; and (c) in the course with face-to-face PCRs, students were generally more positive about the value of PCRs. In light of our findings, we recommend specific ways online PCRs can be better designed.</p>
</p>

<p id="Jacobson2013" class="bibliography"><span class="bibliographykey">Jacobson2013</span>
Ivar Jacobson, Pan-Wei Ng, Paul E. McMahon, Ian Spence, and Svante Lidman:
<em>The Essence of Software Engineering: Applying the SEMAT Kernel</em>.
Addison-Wesley Professional, 2013, 978-0321885951.
<p class="abstract">SEMAT (Software Engineering Methods and Theory) is an international initiative designed to identify a common ground, or universal standard, for software engineering. It is supported by some of the most distinguished contributors to the field. Creating a simple language to describe methods and practices, the SEMAT team expresses this common ground as a kernel—or framework—of elements essential to all software development. The Essence of Software Engineering introduces this kernel and shows how to apply it when developing software and improving a team's way of working. It is a book for software professionals, not methodologists. Its usefulness to development team members, who need to evaluate and choose the best practices for their work, goes well beyond the description or application of any single method.</p>
</p>

<p id="Jorgensen2011" class="bibliography"><span class="bibliographykey">Jorgensen2011</span>
Magne Jørgensen and Stein Grimstad:
"<a href="https://doi.org/10.1109/tse.2010.78">The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment</a>".
<em>IEEE Transactions on Software Engineering</em>, 37(5), 2011,
<a class="doi" href="https://doi.org/10.1109/tse.2010.78">10.1109/tse.2010.78</a>.
<p class="abstract">Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.</p>
</p>

<p id="Jorgensen2012" class="bibliography"><span class="bibliographykey">Jorgensen2012</span>
Magne Jørgensen and Stein Grimstad:
"<a href="https://doi.org/10.1109/tse.2011.40">Software Development Estimation Biases: The Role of Interdependence</a>".
<em>IEEE Transactions on Software Engineering</em>, 38(3), 2012,
<a class="doi" href="https://doi.org/10.1109/tse.2011.40">10.1109/tse.2011.40</a>.
<p class="abstract">Software development effort estimates are frequently too low, which may lead to poor project plans and project failures. One reason for this bias seems to be that the effort estimates produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire a better understanding of the underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased significantly with higher levels of interdependence, i.e., with stronger emphasis on connectedness, social context, and relationships. We propose that this connection may be enabled by an activation of one's self-construal when engaging in effort estimation, and a connection between a more interdependent self-construal and increased search for indirect messages, lower ability to ignore irrelevant context, and a stronger emphasis on socially desirable responses.</p>
</p>

<p id="KanatAlexander2012" class="bibliography"><span class="bibliographykey">KanatAlexander2012</span>
Max Kanat-Alexander:
<em>Code Simplicity: The Science of Software Development</em>.
O'Reilly, 2012, 978-1449313890.
<p class="abstract">Good software development results in simple code. Unfortunately, much of the code existing in the world today is far too complex. This concise guide helps you understand the fundamentals of good software development through universal laws—principles you can apply to any programming language or project from here to eternity.</p>
</p>

<p id="Kapser2008" class="bibliography"><span class="bibliographykey">Kapser2008</span>
Cory J. Kapser and Michael W. Godfrey:
"<a href="https://doi.org/10.1007/s10664-008-9076-6">'Cloning considered harmful' considered harmful: patterns of cloning in software</a>".
<em>Empirical Software Engineering</em>, 13(6), 2008,
<a class="doi" href="https://doi.org/10.1007/s10664-008-9076-6">10.1007/s10664-008-9076-6</a>.
<p class="abstract">Literature on the topic of code cloning often asserts that duplicating code within a software system is a bad practice, that it causes harm to the system's design and should be avoided. However, in our studies, we have found significant evidence that cloning is often used in a variety of ways as a principled engineering tool. For example, one way to evaluate possible new features for a system is to clone the affected subsystems and introduce the new features there, in a kind of sandbox testbed. As features mature and become stable within the experimental subsystems, they can be migrated incrementally into the stable code base; in this way, the risk of introducing instabilities in the stable version is minimized. This paper describes several patterns of cloning that we have observed in our case studies and discusses the advantages and disadvantages associated with using them. We also examine through a case study the frequencies of these clones in two medium-sized open source software systems, the Apache web server and the Gnumeric spreadsheet application. In this study, we found that as many as 71% of the clones could be considered to have a positive impact on the maintainability of the software system.</p>
</p>

<p id="Kasi2013" class="bibliography"><span class="bibliographykey">Kasi2013</span>
Bakhtiar Khan Kasi and Anita Sarma:
"<a href="https://doi.org/10.1109/icse.2013.6606619">Cassandra: Proactive conflict minimization through optimized task scheduling</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606619">10.1109/icse.2013.6606619</a>.
<p class="abstract">Software conflicts arising because of conflicting changes are a regular occurrence and delay projects. The main precept of workspace awareness tools has been to identify potential conflicts early, while changes are still small and easier to resolve. However, in this approach conflicts still occur and require developer time and effort to resolve. We present a novel conflict minimization technique that proactively identifies potential conflicts, encodes them as constraints, and solves the constraint space to recommend a set of conflict-minimal development paths for the team. Here we present a study of four open source projects to characterize the distribution of conflicts and their resolution efforts. We then explain our conflict minimization technique and the design and implementation of this technique in our prototype, Cassandra. We show that Cassandra would have successfully avoided a majority of conflicts in the four open source test subjects. We demonstrate the efficiency of our approach by applying the technique to a simulated set of scenarios with higher than normal incidence of conflicts.</p>
</p>

<p id="Khomh2012" class="bibliography"><span class="bibliographykey">Khomh2012</span>
Foutse Khomh, Tejinder Dhaliwal, Ying Zou, and Bram Adams:
"<a href="https://doi.org/10.1109/msr.2012.6224279">Do faster releases improve software quality? An empirical case study of Mozilla Firefox</a>".
<em>2012 9th IEEE Working Conference on Mining Software Repositories (MSR)</em>, <a class="doi" href="https://doi.org/10.1109/msr.2012.6224279">10.1109/msr.2012.6224279</a>.
<p class="abstract">Nowadays, many software companies are shifting from the traditional 18-month release cycle to shorter release cycles. For example, Google Chrome and Mozilla Firefox release new versions every 6 weeks. These shorter release cycles reduce the users' waiting time for a new release and offer better marketing opportunities to companies, but it is unclear if the quality of the software product improves as well, since shorter release cycles result in shorter testing periods. In this paper, we empirically study the development process of Mozilla Firefox in 2010 and 2011, a period during which the project transitioned to a shorter release cycle. We compare crash rates, median uptime, and the proportion of post-release bugs of the versions that had a shorter release cycle with those having a traditional release cycle, to assess the relation between release cycle length and the software quality observed by the end user. We found that (1) with shorter release cycles, users do not experience significantly more post-release bugs and (2) bugs are fixed faster, yet (3) users experience these bugs earlier during software execution (the program crashes earlier).</p>
</p>

<p id="Kiefer2015" class="bibliography"><span class="bibliographykey">Kiefer2015</span>
Marc Kiefer, Daniel Warzel, and Walter F. Tichy:
"<a href="https://doi.org/10.1145/2837476.2837481">An empirical study on parallelism in modern open-source projects</a>".
<em>Proceedings of the 2nd International Workshop on Software Engineering for Parallel Systems</em>, <a class="doi" href="https://doi.org/10.1145/2837476.2837481">10.1145/2837476.2837481</a>.
<p class="abstract">Writing parallel programs is hard, especially for inexperienced programmers. Parallel language features are still being added on a regular basis to most modern object-oriented languages and this trend is likely to continue. Being able to support developers with tools for writing and optimizing parallel programs requires a deep understanding of how programmers approach and implement parallelism. We present an empirical study of 135 parallel open-source projects in Java, C# and C++ ranging from small (&lt; 1000 lines of code) to very large (&gt; 2M lines of code) codebases. We examine the projects to find out how language features, synchronization mechanisms, parallel data structures and libraries are used by developers to express parallelism. We also determine which common parallel patterns are used and how the implemented solutions compare to typical textbook advice. The results show that similar parallel constructs are used equally often across languages, but usage also heavily depends on how easy to use a certain language feature is. Patterns that do not map well to a language are much rarer compared to other languages. Bad practices are prevalent in hobby projects but also occur in larger projects.</p>
</p>

<p id="Kim2013" class="bibliography"><span class="bibliographykey">Kim2013</span>
Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim:
"<a href="https://doi.org/10.1109/icse.2013.6606626">Automatic patch generation learned from human-written patches</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606626">10.1109/icse.2013.6606626</a>.
<p class="abstract">Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs.</p>
</p>

<p id="Kim2016" class="bibliography"><span class="bibliographykey">Kim2016</span>
Dohyeong Kim, Yonghwi Kwon, Peng Liu, I. Luk Kim, David Mitchel Perry, Xiangyu Zhang, and Gustavo Rodriguez-Rivera:
"<a href="https://doi.org/10.1145/2983990.2984031">Apex: automatic programming assignment error explanation</a>".
<em>Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</em>, <a class="doi" href="https://doi.org/10.1145/2983990.2984031">10.1145/2983990.2984031</a>.
<p class="abstract">This paper presents Apex, a system that can automatically generate explanations for programming assignment bugs, regarding where the bugs are and how the root causes led to the runtime failures. It works by comparing the passing execution of a correct implementation (provided by the instructor) and the failing execution of the buggy implementation (submitted by the student). The technique overcomes a number of technical challenges caused by syntactic and semantic differences of the two implementations. It collects the symbolic traces of the executions and matches assignment statements in the two execution traces by reasoning about symbolic equivalence. It then matches predicates by aligning the control dependences of the matched assignment statements, avoiding direct matching of path conditions which are usually quite different. Our evaluation shows that Apex is every effective for 205 buggy real world student submissions of 4 programming assignments, and a set of 15 programming assignment type of buggy programs collected from stackoverflow.com, precisely pinpointing the root causes and capturing the causality for 94.5% of them. The evaluation on a standard benchmark set with over 700 student bugs shows similar results. A user study in the classroom shows that Apex has substantially improved student productivity.</p>
</p>

<p id="Kinshumann2011" class="bibliography"><span class="bibliographykey">Kinshumann2011</span>
Kinshuman Kinshumann, Kirk Glerum, Steve Greenberg, Gabriel Aul, Vince Orgovan, Greg Nichols, David Grant, Gretchen Loihle, and Galen Hunt:
"<a href="https://doi.org/10.1145/1965724.1965749">Debugging in the (very) large: ten years of implementation and experience</a>".
<em>Communications of the ACM</em>, 54(7), 2011,
<a class="doi" href="https://doi.org/10.1145/1965724.1965749">10.1145/1965724.1965749</a>.
<p class="abstract">Windows Error Reporting (WER) is a distributed system that automates the processing of error reports coming from an installed base of a billion machines. WER has collected billions of error reports in 10 years of operation. It collects error data automatically and classifies errors into buckets, which are used to prioritize developer effort and report fixes to users. WER uses a progressive approach to data collection, which minimizes overhead for most reports yet allows developers to collect detailed information when needed. WER takes advantage of its scale to use error statistics as a tool in debugging; this allows developers to isolate bugs that cannot be found at smaller scale. WER has been designed for efficient operation at large scale: one pair of database servers records all the errors that occur on all Windows computers worldwide.</p>
</p>

<p id="Kocaguneli2012" class="bibliography"><span class="bibliographykey">Kocaguneli2012</span>
Ekrem Kocaguneli, Tim Menzies, and Jacky W. Keung:
"<a href="https://doi.org/10.1109/tse.2011.111">On the Value of Ensemble Effort Estimation</a>".
<em>IEEE Transactions on Software Engineering</em>, 38(6), 2012,
<a class="doi" href="https://doi.org/10.1109/tse.2011.111">10.1109/tse.2011.111</a>.
<p class="abstract">Background: Despite decades of research, there is no consensus on which software effort estimation methods produce the most accurate models. Aim: Prior work has reported that, given M estimation methods, no single method consistently outperforms all others. Perhaps rather than recommending one estimation method as best, it is wiser to generate estimates from ensembles of multiple estimation methods. Method: Nine learners were combined with 10 preprocessing options to generate 9×10 = 90 solo methods. These were applied to 20 datasets and evaluated using seven error measures. This identified the best n (in our case n = 13) solo methods that showed stable performance across multiple datasets and error measures. The top 2, 4, 8, and 13 solo methods were then combined to generate 12 multimethods, which were then compared to the solo methods. Results: 1) The top 10 (out of 12) multimethods significantly outperformed all 90 solo methods. 2) The error rates of the multimethods were significantly less than the solo methods. 3) The ranking of the best multimethod was remarkably stable. Conclusion: While there is no best single effort estimation method, there exist best combinations of such effort estimation methods.</p>
</p>

<p id="Krein2016" class="bibliography"><span class="bibliographykey">Krein2016</span>
Jonathan L. Krein, Lutz Prechelt, Natalia Juristo, Aziz Nanthaamornphong, Jeffrey C. Carver, Sira Vegas, Charles D. Knutson, Kevin D. Seppi, and Dennis L. Eggett:
"<a href="https://doi.org/10.1109/tse.2015.2488625">A Multi-Site Joint Replication of a Design Patterns Experiment Using Moderator Variables to Generalize across Contexts</a>".
<em>IEEE Transactions on Software Engineering</em>, 42(4), 2016,
<a class="doi" href="https://doi.org/10.1109/tse.2015.2488625">10.1109/tse.2015.2488625</a>.
<p class="abstract">Context. Several empirical studies have explored the benefits of software design patterns, but their collective results are highly inconsistent. Resolving the inconsistencies requires investigating moderators—i.e., variables that cause an effect to differ across contexts. Objectives. Replicate a design patterns experiment at multiple sites and identify sufficient moderators to generalize the results across prior studies. Methods. We perform a close replication of an experiment investigating the impact (in terms of time and quality) of design patterns (Decorator and Abstract Factory) on software maintenance. The experiment was replicated once previously, with divergent results. We execute our replication at four universities—spanning two continents and three countries—using a new method for performing distributed replications based on closely coordinated, small-scale instances ("joint replication"). We perform two analyses: 1) a post-hoc analysis of moderators, based on frequentist and Bayesian statistics; 2) an a priori analysis of the original hypotheses, based on frequentist statistics. Results. The main effect differs across the previous instances of the experiment and across the sites in our distributed replication. Our analysis of moderators (including developer experience and pattern knowledge) resolves the differences sufficiently to allow for cross-context (and cross-study) conclusions. The final conclusions represent 126 participants from five universities and 12 software companies, spanning two continents and at least four countries. Conclusions. The Decorator pattern is found to be preferable to a simpler solution during maintenance, as long as the developer has at least some prior knowledge of the pattern. For Abstract Factory, the simpler solution is found to be mostly equivalent to the pattern solution. Abstract Factory is shown to require a higher level of knowledge and/or experience than Decorator for the pattern to be beneficial.</p>
</p>

<p id="Lewis2013" class="bibliography"><span class="bibliographykey">Lewis2013</span>
Chris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, and E. James Whitehead:
"<a href="https://doi.org/10.1109/icse.2013.6606583">Does bug prediction support human developers? Findings from a Google case study</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606583">10.1109/icse.2013.6606583</a>.
<p class="abstract">While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.</p>
</p>

<p id="Li2013" class="bibliography"><span class="bibliographykey">Li2013</span>
Sihan Li, Hucheng Zhou, Haoxiang Lin, Tian Xiao, Haibo Lin, Wei Lin, and Tao Xie:
"<a href="https://doi.org/10.1109/icse.2013.6606646">A characteristic study on failures of production distributed data-parallel programs</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606646">10.1109/icse.2013.6606646</a>.
<p class="abstract">SCOPE is adopted by thousands of developers from tens of different product teams in Microsoft Bing for daily web-scale data processing, including index building, search ranking, and advertisement display. A SCOPE job is composed of declarative SQL-like queries and imperative C# user-defined functions (UDFs), which are executed in pipeline by thousands of machines. There are tens of thousands of SCOPE jobs executed on Microsoft clusters per day, while some of them fail after a long execution time and thus waste tremendous resources. Reducing SCOPE failures would save significant resources. This paper presents a comprehensive characteristic study on 200 SCOPE failures/fixes and 50 SCOPE failures with debugging statistics from Microsoft Bing, investigating not only major failure types, failure sources, and fixes, but also current debugging practice. Our major findings include (1) most of the failures (84.5%) are caused by defects in data processing rather than defects in code logic; (2) table-level failures (22.5%) are mainly caused by programmers' mistakes and frequent data-schema changes while row-level failures (62%) are mainly caused by exceptional data; (3) 93% fixes do not change data processing logic; (4) there are 8% failures with root cause not at the failure-exposing stage, making current debugging practice insufficient in this case. Our study results provide valuable guidelines for future development of data-parallel programs. We believe that these guidelines are not limited to SCOPE, but can also be generalized to other similar data-parallel platforms.</p>
</p>

<p id="Liao2016" class="bibliography"><span class="bibliographykey">Liao2016</span>
Soohyun Nam Liao, Daniel Zingaro, Michael A. Laurenzano, William G. Griswold, and Leo Porter:
"<a href="https://doi.org/10.1145/2960310.2960315">Lightweight, Early Identification of At-Risk CS1 Students</a>".
<em>Proceedings of the 2016 ACM Conference on International Computing Education Research</em>, <a class="doi" href="https://doi.org/10.1145/2960310.2960315">10.1145/2960310.2960315</a>.
<p class="abstract">Being able to identify low-performing students early in the term may help instructors intervene or differently allocate course resources. Prior work in CS1 has demonstrated that clicker correctness in Peer Instruction courses correlates with exam outcomes and, separately, that machine learning models can be built based on early-term programming assessments. This work aims to combine the best elements of each of these approaches. We offer a methodology for creating models, based on in-class clicker questions, to predict cross-term student performance. In as early as week 3 in a 12-week CS1 course, this model is capable of correctly predicting students as being in danger of failing, or not, for 70% of the students, with only 17% of students misclassified as not at-risk when at-risk. Additional measures to ensure more broad applicability of the methodology, along with possible limitations, are explored.</p>
</p>

<p id="Lo2015" class="bibliography"><span class="bibliographykey">Lo2015</span>
David Lo, Nachiappan Nagappan, and Thomas Zimmermann:
"<a href="https://doi.org/10.1145/2786805.2786809">How practitioners perceive the relevance of software engineering research</a>".
<em>Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2786805.2786809">10.1145/2786805.2786809</a>.
<p class="abstract">The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners.</p>
</p>

<p id="Maalej2014" class="bibliography"><span class="bibliographykey">Maalej2014</span>
Walid Maalej, Rebecca Tiarks, Tobias Roehm, and Rainer Koschke:
"<a href="https://doi.org/10.1145/2622669">On the Comprehension of Program Comprehension</a>".
<em>ACM Transactions on Software Engineering and Methodology</em>, 23(4), 2014,
<a class="doi" href="https://doi.org/10.1145/2622669">10.1145/2622669</a>.
<p class="abstract">Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge. We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments. Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.</p>
</p>

<p id="Marinescu2011" class="bibliography"><span class="bibliographykey">Marinescu2011</span>
Cristina Marinescu:
"<a href="https://doi.org/10.1145/2024445.2024456">Are the classes that use exceptions defect prone?</a>".
<em>Proceedings of the 12th international workshop and the 7th annual ERCIM workshop on Principles on software evolution and software evolution - IWPSE-EVOL '11</em>, <a class="doi" href="https://doi.org/10.1145/2024445.2024456">10.1145/2024445.2024456</a>.
<p class="abstract">Exception handling is a mechanism that highlights exceptional functionality of software systems. Currently many empirical studies point out that sometimes developers neglect exceptional functionality, minimizing its importance. In this paper we investigate if the design entities (classes) that use exceptions are more defect prone than the other classes. The results, based on analyzing three releases of Eclipse, show that indeed the classes that use exceptions are more defect prone than the other classes. Based on our results, developers are advertised to pay more attention to the way they handle exceptions.</p>
</p>

<p id="Mattmann2015" class="bibliography"><span class="bibliographykey">Mattmann2015</span>
Chris A. Mattmann, Joshua Garcia, Ivo Krka, Daniel Popescu, and Nenad Medvidović:
"<a href="https://doi.org/10.1007/s10723-015-9324-0">Revisiting the Anatomy and Physiology of the Grid</a>".
<em>Journal of Grid Computing</em>, 13(1), 2015,
<a class="doi" href="https://doi.org/10.1007/s10723-015-9324-0">10.1007/s10723-015-9324-0</a>.
<p class="abstract">A domain-specific software architecture (DSSA) represents an effective, generalized, reusable solution to constructing software systems within a given application domain. In this paper, we revisit the widely cited DSSA for the domain of grid computing. We have studied systems in this domain over the last ten years. During this time, we have repeatedly observed that, while individual grid systems are widely used and deemed successful, the grid DSSA is actually underspecified to the point where providing a precise answer regarding what makes a software system a grid system is nearly impossible. Moreover, every one of the existing purported grid technologies actually violates the published grid DSSA. In response to this, based on an analysis of the source code, documentation, and usage of eighteen of the most pervasive grid technologies, we have significantly refined the original grid DSSA. We demonstrate that this DSSA much more closely matches the grid technologies studied. Our refinements allow us to more definitively identify a software system as a grid technology, and distinguish it from software libraries, middleware, and frameworks.</p>
</p>

<p id="McGee2011" class="bibliography"><span class="bibliographykey">McGee2011</span>
Sharon McGee and Des Greer:
"<a href="https://doi.org/10.1109/re.2011.6051641">Software requirements change taxonomy: Evaluation by case study</a>".
<em>2011 IEEE 19th International Requirements Engineering Conference</em>, <a class="doi" href="https://doi.org/10.1109/re.2011.6051641">10.1109/re.2011.6051641</a>.
<p class="abstract">Although a number of requirements change classifications have been proposed in the literature, there is no empirical assessment of their practical value in terms of their capacity to inform change monitoring and management. This paper describes an investigation of the informative efficacy of a taxonomy of requirements change sources which distinguishes between changes arising from 'market', 'organisation', 'project vision', 'specification' and 'solution'. This investigation was effected through a case study where change data was recorded over a 16 month period covering the development lifecycle of a government sector software application. While insufficiency of data precluded an investigation of changes arising due to the change source of 'market', for the remainder of the change sources, results indicate a significant difference in cost, value to the customer and management considerations. Findings show that higher cost and value changes arose more often from 'organisation' and 'vision' sources; these changes also generally involved the co-operation of more stakeholder groups and were considered to be less controllable than changes arising from the 'specification' or 'solution' sources. Overall, the results suggest that monitoring and measuring change using this classification is a practical means to support change management, understanding and risk visibility.</p>
</p>

<p id="McIntosh2011" class="bibliography"><span class="bibliographykey">McIntosh2011</span>
Shane McIntosh, Bram Adams, Thanh H.D. Nguyen, Yasutaka Kamei, and Ahmed E. Hassan:
"<a href="https://doi.org/10.1145/1985793.1985813">An empirical study of build maintenance effort</a>".
<em>Proceedings of the 33rd International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/1985793.1985813">10.1145/1985793.1985813</a>.
<p class="abstract">The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system specifications require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of different sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the difference in scale, the build system churn rate is comparable to that of the source code, and build changes induce more relative churn on the build system than source code changes induce on the source code. Furthermore, build maintenance yields up to a 27% overhead on source code development and a 44% overhead on test development. Up to 79% of source code developers and 89% of test code developers are significantly impacted by build maintenance, yet investment in build experts can reduce the proportion of impacted developers to 22% of source code developers and 24% of test code developers.</p>
</p>

<p id="McLeod2011" class="bibliography"><span class="bibliographykey">McLeod2011</span>
Laurie McLeod and Stephen G. MacDonell:
"<a href="https://doi.org/10.1145/1978802.1978803">Factors that affect software systems development project outcomes</a>".
<em>ACM Computing Surveys</em>, 43(4), 2011,
<a class="doi" href="https://doi.org/10.1145/1978802.1978803">10.1145/1978802.1978803</a>.
<p class="abstract">Determining the factors that have an influence on software systems development and deployment project outcomes has been the focus of extensive and ongoing research for more than 30 years. We provide here a survey of the research literature that has addressed this topic in the period 1996--2006, with a particular focus on empirical analyses. On the basis of this survey we present a new classification framework that represents an abstracted and synthesized view of the types of factors that have been asserted as influencing project outcomes.</p>
</p>

<p id="Meneely2011" class="bibliography"><span class="bibliographykey">Meneely2011</span>
Andrew Meneely, Pete Rotella, and Laurie Williams:
"<a href="https://doi.org/10.1145/2025113.2025128">Does adding manpower also affect quality?: an empirical, longitudinal analysis</a>".
<em>Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering - SIGSOFT/FSE '11</em>, <a class="doi" href="https://doi.org/10.1145/2025113.2025128">10.1145/2025113.2025128</a>.
<p class="abstract">With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics, we quantified characteristics of team expansion, including team size, expansion rate, expansion acceleration, and modularity with respect to department designations. We examined statistical correlations between our monthly team-level metrics and monthly product-level metrics. Our results indicate that increased team size and linear growth are correlated with later periods of better product quality. However, periods of accelerated team expansion are correlated with later periods of reduced software quality. Furthermore, our linear regression prediction model based on team metrics was able to predict the product's post-release failure rate within a 95% prediction interval for 38 out of 40 months. Our analysis provides insight for project managers into how the expansion of development teams can impact product quality.</p>
</p>

<p id="Meng2013" class="bibliography"><span class="bibliographykey">Meng2013</span>
Na Meng, Miryung Kim, and Kathryn S. McKinley:
"<a href="https://doi.org/10.1109/icse.2013.6606596">Lase: Locating and applying systematic edits by learning from examples</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606596">10.1109/icse.2013.6606596</a>.
<p class="abstract">Adding features and fixing bugs often require systematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits.</p>
</p>

<p id="Meyer2014" class="bibliography"><span class="bibliographykey">Meyer2014</span>
André N. Meyer, Thomas Fritz, Gail C. Murphy, and Thomas Zimmermann:
"<a href="https://doi.org/10.1145/2635868.2635892">Software developers' perceptions of productivity</a>".
<em>Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2635868.2635892">10.1145/2635868.2635892</a>.
<p class="abstract">The better the software development community becomes at creating software, the more software the world seems to demand. Although there is a large body of research about measuring and investigating productivity from an organizational point of view, there is a paucity of research about how software developers, those at the front-line of software construction, think about, assess and try to improve their productivity. To investigate software developers' perceptions of software development productivity, we conducted two studies: a survey with 379 professional software developers to help elicit themes and an observational study with 11 professional software developers to investigate emergent themes in more detail. In both studies, we found that developers perceive their days as productive when they complete many or big tasks without significant interruptions or context switches. Yet, the observational data we collected shows our participants performed significant task and activity switching while still feeling productive. We analyze such apparent contradictions in our findings and use the analysis to propose ways to better support software developers in a retrospection and improvement of their productivity through the development of new tools and the sharing of best practices.</p>
</p>

<p id="Miller2016" class="bibliography"><span class="bibliographykey">Miller2016</span>
Craig S. Miller and Amber Settle:
"<a href="https://doi.org/10.1145/2960310.2960327">Some Trouble with Transparency: An Analysis of Student Errors with Object-oriented Python</a>".
<em>Proceedings of the 2016 ACM Conference on International Computing Education Research</em>, <a class="doi" href="https://doi.org/10.1145/2960310.2960327">10.1145/2960310.2960327</a>.
<p class="abstract">We investigated implications of transparent mechanisms in the context of an introductory object-oriented programming course using Python. Here transparent mechanisms are those that reveal how the instance object in Python relates to its instance data. We asked students to write a new method for a provided Python class in an attempt to answer two research questions: 1) to what extent do Python's transparent OO mechanisms lead to student difficulties? and 2) what are common pitfalls in OO programming using Python that instructors should address? Our methodology also presented the correct answer to the students and solicited their comments on their submission. We conducted a content analysis to classify errors in the student submissions. We find that most students had difficulty with the instance (self) object, either by omitting the parameter in the method definition, by failing to use the instance object when referencing attributes of the object, or both. Reference errors in general were more common than other errors, including misplaced returns and indentation errors. These issues may be connected to problems with parameter passing and using dot-notation, which we argue are prerequisites for OO development in Python.</p>
</p>

<p id="Mockus2010" class="bibliography"><span class="bibliographykey">Mockus2010</span>
Audris Mockus:
"<a href="https://doi.org/10.1145/1882291.1882311">Organizational volatility and its effects on software defects</a>".
<em>Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering - FSE'10</em>, <a class="doi" href="https://doi.org/10.1145/1882291.1882311">10.1145/1882291.1882311</a>.
<p class="abstract">The key premise of an organization is to allow more efficient production, including production of high quality software. To achieve that, an organization defines roles and reporting relationships. Therefore, changes in organization's structure are likely to affect product's quality. We propose and investigate a relationship between developer-centric measures of organizational change and the probability of customer-reported defects in the context of a large software project. We find that the proximity to an organizational change is significantly associated with reductions in software quality. We also replicate results of several prior studies of software quality supporting findings that code, change, and developer characteristics affect fault-proneness. In contrast to prior studies we find that distributed development decreases quality. Furthermore, recent departures from an organization were associated with increased probability of customer-reported defects, thus demonstrating that in the observed context the organizational change reduces product quality.</p>
</p>

<p id="Moe2010" class="bibliography"><span class="bibliographykey">Moe2010</span>
Nils Brede Moe, Torgeir Dingsøyr, and Tore Dybå:
"<a href="https://doi.org/10.1016/j.infsof.2009.11.004">A teamwork model for understanding an agile team: A case study of a Scrum project</a>".
<em>Information and Software Technology</em>, 52(5), 2010,
<a class="doi" href="https://doi.org/10.1016/j.infsof.2009.11.004">10.1016/j.infsof.2009.11.004</a>.
<p class="abstract">Context: Software development depends significantly on team performance, as does any process that involves human interaction. Objective: Most current development methods argue that teams should self-manage. Our objective is thus to provide a better understanding of the nature of self-managing agile teams, and the teamwork challenges that arise when introducing such teams. Method: We conducted extensive fieldwork for 9months in a software development company that introduced Scrum. We focused on the human sensemaking, on how mechanisms of teamwork were understood by the people involved. Results: We describe a project through Dickinson and McIntyre's teamwork model, focusing on the interrelations between essential teamwork components. Problems with team orientation, team leadership and coordination in addition to highly specialized skills and corresponding division of work were important barriers for achieving team effectiveness. Conclusion: Transitioning from individual work to self-managing teams requires a reorientation not only by developers but also by management. This transition takes time and resources, but should not be neglected. In addition to Dickinson and McIntyre's teamwork components, we found trust and shared mental models to be of fundamental importance.</p>
</p>

<p id="Nagappan2008" class="bibliography"><span class="bibliographykey">Nagappan2008</span>
Nachiappan Nagappan, E. Michael Maximilien, Thirumalesh Bhat, and Laurie Williams:
"<a href="https://doi.org/10.1007/s10664-008-9062-z">Realizing quality improvement through test driven development: results and experiences of four industrial teams</a>".
<em>Empirical Software Engineering</em>, 13(3), 2008,
<a class="doi" href="https://doi.org/10.1007/s10664-008-9062-z">10.1007/s10664-008-9062-z</a>.
<p class="abstract">Test-driven development (TDD) is a software development practice that has been used sporadically for decades. With this practice, a software engineer cycles minute-by-minute between writing failing unit tests and writing implementation code to pass those tests. Test-driven development has recently re-emerged as a critical enabling practice of agile software development methodologies. However, little empirical evidence supports or refutes the utility of this practice in an industrial context. Case studies were conducted with three development teams at Microsoft and one at IBM that have adopted TDD. The results of the case studies indicate that the pre-release defect density of the four products decreased between 40% and 90% relative to similar projects that did not use the TDD practice. Subjectively, the teams experienced a 15--35% increase in initial development time after adopting TDD.</p>
</p>

<p id="Nagappan2015" class="bibliography"><span class="bibliographykey">Nagappan2015</span>
Meiyappan Nagappan, Romain Robbes, Yasutaka Kamei, Éric Tanter, Shane McIntosh, Audris Mockus, and Ahmed E. Hassan:
"<a href="https://doi.org/10.1145/2786805.2786834">An empirical study of goto in C code from GitHub repositories</a>".
<em>Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2786805.2786834">10.1145/2786805.2786834</a>.
<p class="abstract">It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is 'harmful' enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically rep- resentative sample of 384 files from a population of almost 250K C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21±5%) and cleaning up resources at the end of a procedure (40.36±5%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was re- moved/modified in the post-release phase of four of the six projects. We conclude that developers limit themselves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice.</p>
</p>

<p id="Nakshatri2016" class="bibliography"><span class="bibliographykey">Nakshatri2016</span>
Suman Nakshatri, Maithri Hegde, and Sahithi Thandra:
"<a href="https://doi.org/10.1145/2901739.2903499">Analysis of exception handling patterns in Java projects</a>".
<em>Proceedings of the 13th International Conference on Mining Software Repositories</em>, <a class="doi" href="https://doi.org/10.1145/2901739.2903499">10.1145/2901739.2903499</a>.
<p class="abstract">Exception handling is a powerful tool provided by many pro- gramming languages to help developers deal with unforeseen conditions. Java is one of the few programming languages to enforce an additional compilation check on certain sub- classes of the Exception class through checked exceptions. As part of this study, empirical data was extracted from soft- ware projects developed in Java. The intent is to explore how developers respond to checked exceptions and identify common patterns used by them to deal with exceptions, checked or otherwise. Bloch's book - "Effective Java" [1] was used as reference for best practices in exception handling - these recommendations were compared against results from the empirical data. Results of this study indicate that most programmers ignore checked exceptions and leave them un- noticed. Additionally, it is observed that classes higher in the exception class hierarchy are more frequently used as compared to specific exception subclasses.</p>
</p>

<p id="Near2016" class="bibliography"><span class="bibliographykey">Near2016</span>
Joseph P. Near and Daniel Jackson:
"<a href="https://doi.org/10.1145/2884781.2884836">Finding security bugs in web applications using a catalog of access control patterns</a>".
<em>Proceedings of the 38th International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2884781.2884836">10.1145/2884781.2884836</a>.
<p class="abstract">We propose a specification-free technique for finding missing security checks in web applications using a catalog of access control patterns in which each pattern models a common access control use case. Our implementation, SPACE, checks that every data exposure allowed by an application's code matches an allowed exposure from a security pattern in our catalog. The only user-provided input is a mapping from application types to the types of the catalog; the rest of the process is entirely automatic. In an evaluation on the 50 most watched Ruby on Rails applications on Github, SPACE reported 33 possible bugs—23 previously unknown security bugs, and 10 false positives.</p>
</p>

<p id="Nussli2012" class="bibliography"><span class="bibliographykey">Nussli2012</span>
Marc-Antoine Nüssli and Patrick Jermann:
"<a href="https://doi.org/10.1145/2145204.2145371">Effects of sharing text selections on gaze cross-recurrence and interaction quality in a pair programming task</a>".
<em>Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work - CSCW '12</em>, <a class="doi" href="https://doi.org/10.1145/2145204.2145371">10.1145/2145204.2145371</a>.
<p class="abstract">We present a dual eye-tracking study that demonstrates the effect of sharing selection among collaborators in a remote pair-programming scenario. Forty pairs of engineering students completed several program understanding tasks while their gaze was synchronously recorded. The coupling of the programmers' focus of attention was measured by a cross-recurrence analysis of gaze that captures how much programmers look at the same sequence of spots within a short time span. A high level of gaze cross-recurrence is typical for pairs who actively engage in grounding efforts to build and maintain shared understanding. As part of their grounding efforts, programmers may use text selection to perform collaborative references. Broadcast selections serve as indexing sites for the selector as they attract non-selector's gaze shortly after they become visible. Gaze cross-recurrence is highest when selectors accompany their selections with speech to produce a multimodal reference.</p>
</p>

<p id="Pankratius2012" class="bibliography"><span class="bibliographykey">Pankratius2012</span>
Victor Pankratius, Felix Schmidt, and Gilda Garreton:
"<a href="https://doi.org/10.1109/icse.2012.6227200">Combining functional and imperative programming for multicore software: An empirical study evaluating Scala and Java</a>".
<em>2012 34th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2012.6227200">10.1109/icse.2012.6227200</a>.
<p class="abstract">Recent multi-paradigm programming languages combine functional and imperative programming styles to make software development easier. Given today's proliferation of multicore processors, parallel programmers are supposed to benefit from this combination, as many difficult problems can be expressed more easily in a functional style while others match an imperative style. Due to a lack of empirical evidence from controlled studies, however, important software engineering questions are largely unanswered. Our paper is the first to provide thorough empirical results by using Scala and Java as a vehicle in a controlled comparative study on multicore software development. Scala combines functional and imperative programming while Java focuses on imperative shared-memory programming. We study thirteen programmers who worked on three projects, including an industrial application, in both Scala and Java. In addition to the resulting 39 Scala programs and 39 Java programs, we obtain data from an industry software engineer who worked on the same project in Scala. We analyze key issues such as effort, code, language usage, performance, and programmer satisfaction. Contrary to popular belief, the functional style does not lead to bad performance. Average Scala run-times are comparable to Java, lowest run-times are sometimes better, but Java scales better on parallel hardware. We confirm with statistical significance Scala's claim that Scala code is more compact than Java code, but clearly refute other claims of Scala on lower programming effort and lower debugging effort. Our study also provides explanations for these observations and shows directions on how to improve multi-paradigm languages in the future.</p>
</p>

<p id="Parnin2012" class="bibliography"><span class="bibliographykey">Parnin2012</span>
Chris Parnin and Spencer Rugaber:
"<a href="https://doi.org/10.1109/icpc.2012.6240479">Programmer information needs after memory failure</a>".
<em>2012 20th IEEE International Conference on Program Comprehension (ICPC)</em>, <a class="doi" href="https://doi.org/10.1109/icpc.2012.6240479">10.1109/icpc.2012.6240479</a>.
<p class="abstract">Despite its vast capacity and associative powers, the human brain does not deal well with interruptions. Particularly in situations where information density is high, such as during a programming task, recovering from an interruption requires extensive time and effort. Although modern program development environments have begun to recognize this problem, none of these tools take into account the brain's structure and limitations. In this paper, we present a conceptual framework for understanding the strengths and weaknesses of human memory, particularly with respect to it ability to deal with work interruptions. The framework explains empirical results obtained from experiments in which programmers were interrupted while working. Based on the framework, we discuss programmer information needs that development tools must satisfy and suggest several memory aids such tools could provide. We also describe our prototype implementation of these memory aids.</p>
</p>

<p id="Patitsas2016" class="bibliography"><span class="bibliographykey">Patitsas2016</span>
Elizabeth Patitsas, Jesse Berlin, Michelle Craig, and Steve Easterbrook:
"<a href="https://doi.org/10.1145/2960310.2960312">Evidence That Computer Science Grades Are Not Bimodal</a>".
<em>Proceedings of the 2016 ACM Conference on International Computing Education Research</em>, <a class="doi" href="https://doi.org/10.1145/2960310.2960312">10.1145/2960310.2960312</a>.
<p class="abstract">It is commonly thought that CS grades are bimodal. We statistically analyzed 778 distributions of final course grades from a large research university, and found only 5.8% of the distributions passed tests of multimodality. We then devised a psychology experiment to understand why CS educators believe their grades to be bimodal. We showed 53 CS professors a series of histograms displaying ambiguous distributions and asked them to categorize the distributions. A random half of participants were primed to think about the fact that CS grades are commonly thought to be bimodal; these participants were more likely to label ambiguous distributions as "bimodal". Participants were also more likely to label distributions as bimodal if they believed that some students are innately predisposed to do better at CS. These results suggest that bimodal grades are instructional folklore in CS, caused by confirmation bias and instructor beliefs about their students.</p>
</p>

<p id="PerezDeRosso2016" class="bibliography"><span class="bibliographykey">PerezDeRosso2016</span>
Santiago Perez De Rosso and Daniel Jackson:
"<a href="https://doi.org/10.1145/2983990.2984018">Purposes, concepts, misfits, and a redesign of Git</a>".
<em>Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</em>, <a class="doi" href="https://doi.org/10.1145/2983990.2984018">10.1145/2983990.2984018</a>.
<p class="abstract">Git is a widely used version control system that is powerful but complicated. Its complexity may not be an inevitable consequence of its power but rather evidence of flaws in its design. To explore this hypothesis, we analyzed the design of Git using a theory that identifies concepts, purposes, and misfits. Some well-known difficulties with Git are described, and explained as misfits in which underlying concepts fail to meet their intended purpose. Based on this analysis, we designed a reworking of Git (called Gitless) that attempts to remedy these flaws. To correlate misfits with issues reported by users, we conducted a study of Stack Overflow questions. And to determine whether users experienced fewer complications using Gitless in place of Git, we conducted a small user study. Results suggest our approach can be profitable in identifying, analyzing, and fixing design problems.</p>
</p>

<p id="Petre2013" class="bibliography"><span class="bibliographykey">Petre2013</span>
Marian Petre:
"<a href="https://doi.org/10.1109/icse.2013.6606618">UML in practice</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606618">10.1109/icse.2013.6606618</a>.
<p class="abstract">UML has been described by some as "the lingua franca of software engineering". Evidence from industry does not necessarily support such endorsements. How exactly is UML being used in industry - if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of UML use.</p>
</p>

<p id="Philip2012" class="bibliography"><span class="bibliographykey">Philip2012</span>
Kavita Philip, Medha Umarji, Megha Agarwala, Susan Elliott Sim, Rosalva Gallardo-Valencia, Cristina V. Lopes, and Sukanya Ratanotayanon:
"<a href="https://doi.org/10.1145/2145204.2145407">Software reuse through methodical component reuse and amethodical snippet remixing</a>".
<em>Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work - CSCW '12</em>, <a class="doi" href="https://doi.org/10.1145/2145204.2145407">10.1145/2145204.2145407</a>.
<p class="abstract">Every method for developing software is a prescriptive model. Applying a deconstructionist analysis to methods reveals that there are two texts, or sets of assumptions and ideals: a set that is privileged by the method and a second set that is left out, or marginalized by the method. We apply this analytical lens to software reuse, a technique in software development that seeks to expedite one's own project by using programming artifacts created by others. By analyzing the methods prescribed by Component-Based Software Engineering (CBSE), we arrive at two texts: Methodical CBSE and Amethodical Remixing. Empirical data from four studies on code search on the web draws attention to four key points of tension: status of component boundaries; provenance of source code; planning and process; and evaluation criteria for candidate code. We conclude the paper with a discussion of the implications of this work for the limits of methods, structure of organizations that reuse software, and the design of search engines for source code.</p>
</p>

<p id="Porter2013" class="bibliography"><span class="bibliographykey">Porter2013</span>
Leo Porter, Cynthia Bailey Lee, and Beth Simon:
"<a href="https://doi.org/10.1145/2445196.2445250">Halving fail rates using peer instruction</a>".
<em>Proceeding of the 44th ACM technical symposium on Computer science education - SIGCSE '13</em>, <a class="doi" href="https://doi.org/10.1145/2445196.2445250">10.1145/2445196.2445250</a>.
<p class="abstract">Peer Instruction (PI) is a teaching method that supports student-centric classrooms, where students construct their own understanding through a structured approach featuring questions with peer discussions. PI has been shown to increase learning in STEM disciplines such as physics and biology. In this report we look at another indicator of student success the rate at which students pass the course or, conversely, the rate at which they fail. Evaluating 10 years of instruction of 4 different courses spanning 16 PI course instances, we find that adoption of the PI methodology in the classroom reduces fail rates by a per-course average of 61% (20% reduced to 7%) compared to standard instruction (SI). Moreover, we also find statistically significant improvements within-instructor. For the same instructor teaching the same course, we find PI decreases the fail rate, on average, by 67% (from 23% to 8%) compared to SI. As an in-situ study, we discuss the various threats to the validity of this work and consider implications of wide-spread adoption of PI in computing programs.</p>
</p>

<p id="Posnett2011" class="bibliography"><span class="bibliographykey">Posnett2011</span>
Daryl Posnett, Abram Hindle, and Premkumar Devanbu:
"<a href="https://doi.org/10.1109/wcre.2011.33">Got Issues? Do New Features and Code Improvements Affect Defects?</a>".
<em>2011 18th Working Conference on Reverse Engineering</em>, <a class="doi" href="https://doi.org/10.1109/wcre.2011.33">10.1109/wcre.2011.33</a>.
<p class="abstract">There is a perception that when new features are added to a system that those added and modified parts of the source-code are more fault prone. Many have argued that new code and new features are defect prone due to immaturity, lack of testing, as well unstable requirements. Unfortunately most previous work does not investigate the link between a concrete requirement or new feature and the defects it causes, in particular the feature, the changed code and the subsequent defects are rarely investigated. In this paper we investigate the relationship between improvements, new features and defects recorded within an issue tracker. A manual case study is performed to validate the accuracy of these issue types. We combine defect issues and new feature issues with the code from version-control systems that introduces these features, we then explore the relationship of new features with the fault-proneness of their implementations. We describe properties and produce models of the relationship between new features and fault proneness, based on the analysis of issue trackers and version-control systems. We find, surprisingly, that neither improvements nor new features have any significant effect on later defect counts, when controlling for size and total number of changes.</p>
</p>

<p id="Prabhu2011" class="bibliography"><span class="bibliographykey">Prabhu2011</span>
Prakash Prabhu, Yun Zhang, Soumyadeep Ghosh, David I. August, Jialu Huang, Stephen Beard, Hanjun Kim, Taewook Oh, Thomas B. Jablin, Nick P. Johnson, Matthew Zoufaly, Arun Raman, Feng Liu, and David Walker:
"<a href="https://doi.org/10.1145/2063348.2063374">A survey of the practice of computational science</a>".
<em>State of the Practice Reports on - SC '11</em>, <a class="doi" href="https://doi.org/10.1145/2063348.2063374">10.1145/2063348.2063374</a>.
<p class="abstract">Computing plays an indispensable role in scientific research. Presently, researchers in science have different problems, needs, and beliefs about computation than professional programmers. In order to accelerate the progress of science, computer scientists must understand these problems, needs, and beliefs. To this end, this paper presents a survey of scientists from diverse disciplines, practicing computational science at a doctoral-granting university with very high re search activity. The survey covers many things, among them, prevalent programming practices within this scientific community, the importance of computational power in different fields, use of tools to enhance performance and soft ware productivity, computational resources leveraged, and prevalence of parallel computation. The results reveal several patterns that suggest interesting avenues to bridge the gap between scientific researchers and programming tools developers.</p>
</p>

<p id="Pritchard2015" class="bibliography"><span class="bibliographykey">Pritchard2015</span>
David Pritchard:
"<a href="https://doi.org/10.1145/2846680.2846681">Frequency distribution of error messages</a>".
<em>Proceedings of the 6th Workshop on Evaluation and Usability of Programming Languages and Tools</em>, <a class="doi" href="https://doi.org/10.1145/2846680.2846681">10.1145/2846680.2846681</a>.
<p class="abstract">Which programming error messages are the most common? We investigate this question, motivated by writing error explanations for novices. We consider large data sets in Python and Java that include both syntax and run-time errors. In both data sets, after grouping essentially identical messages, the error message frequencies empirically resemble Zipf-Mandelbrot distributions. We use a maximum-likelihood approach to fit the distribution parameters. This gives one possible way to contrast languages or compilers quantitatively.</p>
</p>

<p id="Racheva2010" class="bibliography"><span class="bibliographykey">Racheva2010</span>
Zornitza Racheva, Maya Daneva, Klaas Sikkel, Andrea Herrmann, and Roel Wieringa:
"<a href="https://doi.org/10.1109/re.2010.27">Do We Know Enough about Requirements Prioritization in Agile Projects: Insights from a Case Study</a>".
<em>2010 18th IEEE International Requirements Engineering Conference</em>, <a class="doi" href="https://doi.org/10.1109/re.2010.27">10.1109/re.2010.27</a>.
<p class="abstract">Requirements prioritization is an essential mechanism of agile software development approaches. It maximizes the value delivered to the clients and accommodates changing requirements. This paper presents results of an exploratory cross-case study on agile prioritization and business value delivery processes in eight software organizations. We found that some explicit and fundamental assumptions of agile requirement prioritization approaches, as described in the agile literature on best practices, do not hold in all agile project contexts in our study. These are (i) the driving role of the client in the value creation process, (ii) the prevailing position of business value as a main prioritization criterion, (iii) the role of the prioritization process for project goal achievement. This implies that these assumptions have to be reframed and that the approaches to requirements prioritization for value creation need to be extended.</p>
</p>

<p id="Rahman2011" class="bibliography"><span class="bibliographykey">Rahman2011</span>
Foyzur Rahman and Premkumar Devanbu:
"<a href="https://doi.org/10.1145/1985793.1985860">Ownership, experience and defects: a fine-grained study of authorship</a>".
<em>Proceedings of the 33rd International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/1985793.1985860">10.1145/1985793.1985860</a>.
<p class="abstract">Recent research indicates that "people" factors such as ownership, experience, organizational structure, and geographic distribution have a big impact on software quality. Understanding these factors, and properly deploying people resources can help managers improve quality outcomes. This paper considers the impact of code ownership and developer experience on software quality. In a large project, a file might be entirely owned by a single developer, or worked on by many. Some previous research indicates that more developers working on a file might lead to more defects. Prior research considered this phenomenon at the level of modules or files, and thus does not tease apart and study the effect of contributions of different developers to each module or file. We exploit a modern version control system to examine this issue at a fine-grained level. Using version history, we examine contributions to code fragments that are actually repaired to fix bugs. Are these code fragments "implicated" in bugs the result of contributions from many? or from one? Does experience matter? What type of experience? We find that implicated code is more strongly associated with a single developer's contribution; our findings also indicate that an author's specialized experience in the target file is more important than general experience. Our findings suggest that quality control efforts could be profitably targeted at changes made by single developers with limited prior experience on that file.</p>
</p>

<p id="Rahman2013" class="bibliography"><span class="bibliographykey">Rahman2013</span>
Foyzur Rahman and Premkumar Devanbu:
"<a href="https://doi.org/10.1109/icse.2013.6606589">How, and why, process metrics are better</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606589">10.1109/icse.2013.6606589</a>.
<p class="abstract">Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.</p>
</p>

<p id="Rigby2011" class="bibliography"><span class="bibliographykey">Rigby2011</span>
Peter C. Rigby and Margaret-Anne Storey:
"<a href="https://doi.org/10.1145/1985793.1985867">Understanding broadcast based peer review on open source software projects</a>".
<em>Proceedings of the 33rd International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/1985793.1985867">10.1145/1985793.1985867</a>.
<p class="abstract">Software peer review has proven to be a successful technique in open source software (OSS) development. In contrast to industry, where reviews are typically assigned to specific individuals, changes are broadcast to hundreds of potentially interested stakeholders. Despite concerns that reviews may be ignored, or that discussions will deadlock because too many uninformed stakeholders are involved, we find that this approach works well in practice. In this paper, we describe an empirical study to investigate the mechanisms and behaviours that developers use to find code changes they are competent to review. We also explore how stakeholders interact with one another during the review process. We manually examine hundreds of reviews across five high profile OSS projects. Our findings provide insights into the simple, community-wide techniques that developers use to effectively manage large quantities of reviews. The themes that emerge from our study are enriched and validated by interviewing long-serving core developers.</p>
</p>

<p id="Rivers2016" class="bibliography"><span class="bibliographykey">Rivers2016</span>
Kelly Rivers, Erik Harpstead, and Ken Koedinger:
"<a href="https://doi.org/10.1145/2960310.2960333">Learning Curve Analysis for Programming</a>".
<em>Proceedings of the 2016 ACM Conference on International Computing Education Research</em>, <a class="doi" href="https://doi.org/10.1145/2960310.2960333">10.1145/2960310.2960333</a>.
<p class="abstract">The recent surge in interest in using educational data mining on student written programs has led to discoveries about which compiler errors students encounter while they are learning how to program. However, less attention has been paid to the actual code that students produce. In this paper, we investigate programming data by using learning curve analysis to determine which programming elements students struggle with the most when learning in Python. Our analysis extends the traditional use of learning curve analysis to include less structured data, and also reveals new possibilities for when to teach students new programming concepts. One particular discovery is that while we find evidence of student learning in some cases (for example, in function definitions and comparisons), there are other programming elements which do not demonstrate typical learning. In those cases, we discuss how further changes to the model could affect both demonstrated learning and our understanding of the different concepts that students learn.</p>
</p>

<p id="Robillard2010" class="bibliography"><span class="bibliographykey">Robillard2010</span>
Martin P. Robillard and Rob DeLine:
"<a href="https://doi.org/10.1007/s10664-010-9150-8">A field study of API learning obstacles</a>".
<em>Empirical Software Engineering</em>, 16(6), 2010,
<a class="doi" href="https://doi.org/10.1007/s10664-010-9150-8">10.1007/s10664-010-9150-8</a>.
<p class="abstract">Large APIs can be hard to learn, and this can lead to decreased programmer productivity. But what makes APIs hard to learn? We conducted a mixed approach, multi-phased study of the obstacles faced by Microsoft developers learning a wide variety of new APIs. The study involved a combination of surveys and in-person interviews, and collected the opinions and experiences of over 440 professional developers. We found that some of the most severe obstacles faced by developers learning new APIs pertained to the documentation and other learning resources. We report on the obstacles developers face when learning new APIs, with a special focus on obstacles related to API documentation. Our qualitative analysis elicited five important factors to consider when designing API documentation: documentation of intent; code examples; matching APIs with scenarios; penetrability of the API; and format and presentation. We analyzed how these factors can be interpreted to prioritize API documentation development efforts</p>
</p>

<p id="Rossbach2010" class="bibliography"><span class="bibliographykey">Rossbach2010</span>
Christopher J. Rossbach, Owen S. Hofmann, and Emmett Witchel:
"<a href="https://doi.org/10.1145/1837853.1693462">Is transactional programming actually easier?</a>".
<em>ACM SIGPLAN Notices</em>, 45(5), 2010,
<a class="doi" href="https://doi.org/10.1145/1837853.1693462">10.1145/1837853.1693462</a>.
<p class="abstract">Chip multi-processors (CMPs) have become ubiquitous, while tools that ease concurrent programming have not. The promise of increased performance for all applications through ever more parallel hardware requires good tools for concurrent programming, especially for average programmers. Transactional memory (TM) has enjoyed recent interest as a tool that can help programmers program concurrently. The transactional memory (TM) research community is heavily invested in the claim that programming with transactional memory is easier than alternatives (like locks), but evidence for or against the veracity of this claim is scant. In this paper, we describe a user-study in which 237 undergraduate students in an operating systems course implement the same programs using coarse and fine-grain locks, monitors, and transactions. We surveyed the students after the assignment, and examined their code to determine the types and frequency of programming errors for each synchronization technique. Inexperienced programmers found baroque syntax a barrier to entry for transactional programming. On average, subjective evaluation showed that students found transactions harder to use than coarse-grain locks, but slightly easier to use than fine-grained locks. Detailed examination of synchronization errors in the students' code tells a rather different story. Overwhelmingly, the number and types of programming errors the students made was much lower for transactions than for locks. On a similar programming problem, over 70% of students made errors with fine-grained locking, while less than 10% made errors with transactions.</p>
</p>

<p id="Scanniello2017" class="bibliography"><span class="bibliographykey">Scanniello2017</span>
Giuseppe Scanniello, Michele Risi, Porfirio Tramontana, and Simone Romano:
"<a href="https://doi.org/10.1145/3104029">Fixing Faults in C and Java Source Code</a>".
<em>ACM Transactions on Software Engineering and Methodology</em>, 26(2), 2017,
<a class="doi" href="https://doi.org/10.1145/3104029">10.1145/3104029</a>.
<p class="abstract">We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or baseline) controlled experiment and three replications. We involved 100 participants with different backgrounds and experiences in total. Overall results suggested that there is no difference in terms of effort, effectiveness, and efficiency to fix faults, when source code contains either only abbreviated or only full-word identifier names. We also conducted a qualitative study to understand the values, beliefs, and assumptions that inform and shape fault fixing when identifier names are either abbreviated or full-word. We involved in this qualitative study six professional developers with 1--3 years of work experience. A number of insights emerged from this qualitative study and can be considered a useful complement to the quantitative results from our family of experiments. One of the most interesting insights is that developers, when working on source code with abbreviated identifier names, adopt a more methodical approach to identify and fix faults by extending their focus point and only in a few cases do they expand abbreviated identifiers.</p>
</p>

<p id="Sharp2016" class="bibliography"><span class="bibliographykey">Sharp2016</span>
Helen Sharp, Yvonne Dittrich, and Cleidson R. B. de Souza:
"<a href="https://doi.org/10.1109/tse.2016.2519887">The Role of Ethnographic Studies in Empirical Software Engineering</a>".
<em>IEEE Transactions on Software Engineering</em>, 42(8), 2016,
<a class="doi" href="https://doi.org/10.1109/tse.2016.2519887">10.1109/tse.2016.2519887</a>.
<p class="abstract">Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.</p>
</p>

<p id="Staples2013" class="bibliography"><span class="bibliographykey">Staples2013</span>
Mark Staples, Rafal Kolanski, Gerwin Klein, Corey Lewis, June Andronick, Toby Murray, Ross Jeffery, and Len Bass:
"<a href="https://doi.org/10.1109/icse.2013.6606692">Formal specifications better than function points for code sizing</a>".
<em>2013 35th International Conference on Software Engineering (ICSE)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2013.6606692">10.1109/icse.2013.6606692</a>.
<p class="abstract">Size and effort estimation is a significant challenge for the management of large-scale formal verification projects. We report on an initial study of relationships between the sizes of artefacts from the development of seL4, a formally-verified embedded systems microkernel. For each API function we first determined its COSMIC Function Point (CFP) count (based on the seL4 user manual), then sliced the formal specifications and source code, and performed a normalised line count on these artefact slices. We found strong and significant relationships between the sizes of the artefact slices, but no significant relationships between them and the CFP counts. Our finding that CFP is poorly correlated with lines of code is based on just one system, but is largely consistent with prior literature. We find CFP is also poorly correlated with the size of formal specifications. Nonetheless, lines of formal specification correlate with lines of source code, and this may provide a basis for size prediction in future formal verification projects. In future work we will investigate proof sizing.</p>
</p>

<p id="Stefik2011" class="bibliography"><span class="bibliographykey">Stefik2011</span>
Andreas Stefik, Susanna Siebert, Melissa Stefik, and Kim Slattery:
"<a href="https://doi.org/10.1145/2089155.2089159">An empirical comparison of the accuracy rates of novices using the Quorum, Perl, and Randomo programming languages</a>".
<em>Proceedings of the 3rd ACM SIGPLAN workshop on Evaluation and usability of programming languages and tools - PLATEAU '11</em>, <a class="doi" href="https://doi.org/10.1145/2089155.2089159">10.1145/2089155.2089159</a>.
<p class="abstract">We present here an empirical study comparing the accuracy rates of novices writing software in three programming languages: Quorum, Perl, and Randomo. The first language, Quorum, we call an evidence-based programming language, where the syntax, semantics, and API designs change in correspondence to the latest academic research and literature on programming language usability. Second, while Perl is well known, we call Randomo a Placebo-language, where some of the syntax was chosen with a random number generator and the ASCII table. We compared novices that were programming for the first time using each of these languages, testing how accurately they could write simple programs using common program constructs (e.g., loops, conditionals, functions, variables, parameters). Results showed that while Quorum users were afforded significantly greater accuracy compared to those using Perl and Randomo, Perl users were unable to write programs more accurately than those using a language designed by chance.</p>
</p>

<p id="Stefik2013" class="bibliography"><span class="bibliographykey">Stefik2013</span>
Andreas Stefik and Susanna Siebert:
"<a href="https://doi.org/10.1145/2534973">An Empirical Investigation into Programming Language Syntax</a>".
<em>ACM Transactions on Computing Education</em>, 13(4), 2013,
<a class="doi" href="https://doi.org/10.1145/2534973">10.1145/2534973</a>.
<p class="abstract">Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.</p>
</p>

<p id="Stolee2011" class="bibliography"><span class="bibliographykey">Stolee2011</span>
Kathryn T. Stolee and Sebastian Elbaum:
"<a href="https://doi.org/10.1145/1985793.1985805">Refactoring pipe-like mashups for end-user programmers</a>".
<em>Proceedings of the 33rd International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/1985793.1985805">10.1145/1985793.1985805</a>.
<p class="abstract">Mashups are becoming increasingly popular as end users are able to easily access, manipulate, and compose data from many web sources. We have observed, however, that mashups tend to suffer from deficiencies that propagate as mashups are reused. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on end-user programmers and observe that users generally prefer mashups without smells. We then introduce refactorings targeting those smells, reducing the complexity of the mashup programs, increasing their abstraction, updating broken data sources and dated components, and standardizing their structures to fit the community development patterns. Our assessment of a large sample of mashups shows that smells are present in 81% of them and that the proposed refactorings can reduce the number of smelly mashups to 16%, illustrating the potential of refactoring to support the thousands of end users programming mashups.</p>
</p>

<p id="Stylos2007" class="bibliography"><span class="bibliographykey">Stylos2007</span>
Jeffrey Stylos and Steven Clarke:
"<a href="https://doi.org/10.1109/icse.2007.92">Usability Implications of Requiring Parameters in Objects' Constructors</a>".
<em>29th International Conference on Software Engineering (ICSE'07)</em>, <a class="doi" href="https://doi.org/10.1109/icse.2007.92">10.1109/icse.2007.92</a>.
<p class="abstract">The usability of APIs is increasingly important to programmer productivity. Based on experience with usability studies of specific APIs, techniques were explored for studying the usability of design choices common to many APIs. A comparative study was performed to assess how professional programmers use APIs with required parameters in objects' constructors as opposed to parameterless "default" constructors. It was hypothesized that required parameters would create more usable and self- documenting APIs by guiding programmers toward the correct use of objects and preventing errors. However, in the study, it was found that, contrary to expectations, programmers strongly preferred and were more effective with APIs that did not require constructor parameters. Participants' behavior was analyzed using the cognitive dimensions framework, and revealing that required constructor parameters interfere with common learning strategies, causing undesirable premature commitment.</p>
</p>

<p id="Tew2011" class="bibliography"><span class="bibliographykey">Tew2011</span>
Allison Elliott Tew and Mark Guzdial:
"<a href="https://doi.org/10.1145/1953163.1953200">The FCS1: a language independent assessment of CS1 knowledge</a>".
<em>Proceedings of the 42nd ACM technical symposium on Computer science education - SIGCSE '11</em>, <a class="doi" href="https://doi.org/10.1145/1953163.1953200">10.1145/1953163.1953200</a>.
<p class="abstract">A primary goal of many CS education projects is to determine the extent to which a given intervention has had an impact on student learning. However, computing lacks valid assessments for pedagogical or research purposes. Without such valid assessments, it is difficult to accurately measure student learning or establish a relationship between the instructional setting and learning outcomes. We developed the Foundational CS1 (FCS1) Assessment instrument, the first assessment instrument for introductory computer science concepts that is applicable across a variety of current pedagogies and programming languages. We applied methods from educational and psychological test development, adapting them as necessary to fit the disciplinary context. We conducted a large scale empirical study to demonstrate that pseudo-code was an appropriate mechanism for achieving programming language independence. Finally, we established the validity of the assessment using a multi-faceted argument, combining interview data, statistical analysis of results on the assessment, and CS1 exam scores.</p>
</p>

<p id="Thongtanunam2016" class="bibliography"><span class="bibliographykey">Thongtanunam2016</span>
Patanamon Thongtanunam, Shane McIntosh, Ahmed E. Hassan, and Hajimu Iida:
"<a href="https://doi.org/10.1145/2884781.2884852">Revisiting code ownership and its relationship with software quality in the scope of modern code review</a>".
<em>Proceedings of the 38th International Conference on Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2884781.2884852">10.1145/2884781.2884852</a>.
<p class="abstract">Code ownership establishes a chain of responsibility for modules in large software systems. Although prior work uncovers a link between code ownership heuristics and software quality, these heuristics rely solely on the authorship of code changes. In addition to authoring code changes, developers also make important contributions to a module by reviewing code changes. Indeed, recent work shows that reviewers are highly active in modern code review processes, often suggesting alternative solutions or providing updates to the code changes. In this paper, we complement traditional code ownership heuristics using code review activity. Through a case study of six releases of the large Qt and OpenStack systems, we find that: (1) 67%-86% of developers did not author any code changes for a module, but still actively contributed by reviewing 21%-39% of the code changes, (2) code ownership heuristics that are aware of reviewing activity share a relationship with software quality, and (3) the proportion of reviewers without expertise shares a strong, increasing relationship with the likelihood of having post-release defects. Our results suggest that reviewing activity captures an important aspect of code ownership, and should be included in approximations of it in future studies.</p>
</p>

<p id="Vanhanen2007" class="bibliography"><span class="bibliographykey">Vanhanen2007</span>
Jari Vanhanen and Harri Korpi:
"<a href="https://doi.org/10.1109/hicss.2007.218">Experiences of Using Pair Programming in an Agile Project</a>".
<em>2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)</em>, <a class="doi" href="https://doi.org/10.1109/hicss.2007.218">10.1109/hicss.2007.218</a>.
<p class="abstract">The interest in pair programming (PP) has increased recently, e.g. by the popularization of agile software development. However, many practicalities of PP are poorly understood. We present experiences of using PP extensively in an industrial project. The fact that the team had a limited number of high-end workstations forced it in a positive way to quick deployment and rigorous use of PP. The developers liked PP and learned it easily. Initially, the pairs were not rotated frequently but adopting daily, random rotation improved the situation. Frequent rotation seemed to improve knowledge transfer. The driver/navigator roles were switched seldom, but still the partners communicated actively. The navigator rarely spotted defects during coding, but the released code contained almost no defects. Test-driven development and design in pairs possibly decreased defects. The developers considered that PP improved quality and knowledge transfer, and was better suited for complex tasks than for easy tasks</p>
</p>

<p id="Wang2016" class="bibliography"><span class="bibliographykey">Wang2016</span>
Xinyu Wang, Sumit Gulwani, and Rishabh Singh:
"<a href="https://doi.org/10.1145/2983990.2984030">FIDEX: filtering spreadsheet data using examples</a>".
<em>Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications</em>, <a class="doi" href="https://doi.org/10.1145/2983990.2984030">10.1145/2983990.2984030</a>.
<p class="abstract">Data filtering in spreadsheets is a common problem faced by millions of end-users. The task of data filtering requires a computational model that can separate intended positive and negative string instances. We present a system, FIDEX, that can efficiently learn desired data filtering expressions from a small set of positive and negative string examples. There are two key ideas of our approach. First, we design an expressive DSL to represent disjunctive filter expressions needed for several real-world data filtering tasks. Second, we develop an efficient synthesis algorithm for incrementally learning consistent filter expressions in the DSL from very few positive and negative examples. A DAG-based data structure is used to succinctly represent a large number of filter expressions, and two corresponding operators are defined for algorithmically handling positive and negative examples, namely, the intersection and subtraction operators. FIDEX is able to learn data filters for 452 out of 460 real-world data filtering tasks in real time (0.22s), using only 2.2 positive string instances and 2.7 negative string instances on average.</p>
</p>

<p id="Washburn2016" class="bibliography"><span class="bibliographykey">Washburn2016</span>
Michael Washburn, Pavithra Sathiyanarayanan, Meiyappan Nagappan, Thomas Zimmermann, and Christian Bird:
"<a href="https://doi.org/10.1145/2889160.2889253">What went right and what went wrong: an analysis of 155 postmortems from game development</a>".
<em>Proceedings of the 38th International Conference on Software Engineering Companion</em>, <a class="doi" href="https://doi.org/10.1145/2889160.2889253">10.1145/2889160.2889253</a>.
<p class="abstract">In game development, software teams often conduct postmortems to reflect on what went well and what went wrong in a project. The postmortems are shared publicly on gaming sites or at developer conferences. In this paper, we present an analysis of 155 postmortems published on the gaming site Gamasutra.com. We identify characteristics of game development, link the characteristics to positive and negative experiences in the postmortems and distill a set of best practices and pitfalls for game development.</p>
</p>

<p id="Wicherts2011" class="bibliography"><span class="bibliographykey">Wicherts2011</span>
Jelte M. Wicherts, Marjan Bakker, and Dylan Molenaar:
"<a href="https://doi.org/10.1371/journal.pone.0026828">Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results</a>".
<em>PLoS ONE</em>, 6(11), 2011,
<a class="doi" href="https://doi.org/10.1371/journal.pone.0026828">10.1371/journal.pone.0026828</a>.
<p class="abstract">Background The widespread reluctance to share published research data is often hypothesized to be due to the authors' fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. Methods and Findings We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance. Conclusions Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.</p>
</p>

<p id="Wilkerson2012" class="bibliography"><span class="bibliographykey">Wilkerson2012</span>
Jerod W. Wilkerson, Jay F. Nunamaker, and Rick Mercer:
"<a href="https://doi.org/10.1109/tse.2011.46">Comparing the Defect Reduction Benefits of Code Inspection and Test-Driven Development</a>".
<em>IEEE Transactions on Software Engineering</em>, 38(3), 2012,
<a class="doi" href="https://doi.org/10.1109/tse.2011.46">10.1109/tse.2011.46</a>.
<p class="abstract">This study is a quasi experiment comparing the software defect rates and implementation costs of two methods of software defect reduction: code inspection and test-driven development. We divided participants, consisting of junior and senior computer science students at a large Southwestern university, into four groups using a two-by-two, between-subjects, factorial design and asked them to complete the same programming assignment using either test-driven development, code inspection, both, or neither. We compared resulting defect counts and implementation costs across groups. We found that code inspection is more effective than test-driven development at reducing defects, but that code inspection is also more expensive. We also found that test-driven development was no more effective at reducing defects than traditional programming methods.</p>
</p>

<p id="Xu2015" class="bibliography"><span class="bibliographykey">Xu2015</span>
Tianyin Xu, Long Jin, Xuepeng Fan, Yuanyuan Zhou, Shankar Pasupathy, and Rukma Talwadker:
"<a href="https://doi.org/10.1145/2786805.2786852">Hey, you have given me too many knobs!: understanding and dealing with over-designed configuration in system software</a>".
<em>Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering</em>, <a class="doi" href="https://doi.org/10.1145/2786805.2786852">10.1145/2786805.2786852</a>.
<p class="abstract">Configuration problems are not only prevalent, but also severely impair the reliability of today's system software. One fundamental reason is the ever-increasing complexity of configuration, reflected by the large number of configuration parameters ("knobs"). With hundreds of knobs, configuring system software to ensure high reliability and performance becomes a daunting, error-prone task. This paper makes a first step in understanding a fundamental question of configuration design: "do users really need so many knobs?" To provide the quantitatively answer, we study the configuration settings of real-world users, including thousands of customers of a commercial storage system (Storage-A), and hundreds of users of two widely-used open-source system software projects. Our study reveals a series of interesting findings to motivate software architects and developers to be more cautious and disciplined in configuration design. Motivated by these findings, we provide a few concrete, practical guidelines which can significantly reduce the configuration space. Take Storage-A as an example, the guidelines can remove 51.9% of its parameters and simplify 19.7% of the remaining ones with little impact on existing users. Also, we study the existing configuration navigation methods in the context of "too many knobs" to understand their effectiveness in dealing with the over-designed configuration, and to provide practices for building navigation support in system software.</p>
</p>

<p id="Yin2011" class="bibliography"><span class="bibliographykey">Yin2011</span>
Zuoning Yin, Ding Yuan, Yuanyuan Zhou, Shankar Pasupathy, and Lakshmi Bairavasundaram:
"<a href="https://doi.org/10.1145/2025113.2025121">How do fixes become bugs?</a>".
<em>Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering - SIGSOFT/FSE '11</em>, <a class="doi" href="https://doi.org/10.1145/2025113.2025121">10.1145/2025113.2025121</a>.
<p class="abstract">Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation. This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include: (1) at least 14.8%--24.4% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly: 39% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process.</p>
</p>

<p id="Yuan2014" class="bibliography"><span class="bibliographykey">Yuan2014</span>
Ding Yuan, Yu Luo, Xin Zhuang, Guilherme Renna Rodrigues, Xu Zhao, Pranay U. Jain, and Michael Stumm:
"<a href="https://doi.org/10.13140/2.1.2044.2889">Simple Testing Can Prevent Most Critical Failures—An Analysis of Production Failures in Distributed Data-intensive Systems</a>".
<em>11th USENIX Symposium on Operating System Design and Implementation (OSDI'14)</em>, <a class="doi" href="https://doi.org/10.13140/2.1.2044.2889">10.13140/2.1.2044.2889</a>.
<p class="abstract">Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failures. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufficient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures—often with unit tests. We found the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code—the last line of defense—even without an understanding of the software design. We extracted three simple rules from the bugs that have lead to some of the catastrophic failures, and developed a static checker, Aspirator, capable of locating these bugs. Over 30% of the catastrophic failures would have been prevented had Aspirator been used and the identified bugs fixed. Running Aspirator on the code of 9 distributed systems located 143 bugs and bad practices that have been fixed or confirmed by the developers.</p>
</p>

